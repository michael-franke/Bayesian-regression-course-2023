{
  "hash": "dac04dd332109bbcd3f63f300557bc6f",
  "result": {
    "markdown": "---\ntitle: \"Bayesian regression: theory & practice\"\nsubtitle: \"02c: Categorical predictors (exercises)\"\nauthor: \"Michael Franke & Timo Roettger\"\nformat: html\neditor: visual\nexecute:\n  error: false\n  warning: false\n  message: false\ncallout-appearance: simple\n---\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin <- aida::data_MT\n```\n:::\n\n\n# Regression w/ multiple categorical predictors\n\nWe want to regress `log RT` against the full combination of categorical factors `group`, `condition`, and `prototype_label`.\n\n`log RT ~ group * condition * prototype_label`\n\nThe research hypotheses we would like to investigate are:\n\n1.  Typical trials are faster than atypical ones.\n2.  CoM trials are slower than the other kinds of trials (straight and curved) together, and respectively.\n3.  'straight' trials are faster than 'curved' trials.\n4.  Click trials are slower than touch trials.\n\nBut for this to work (without at least mildly informative priors), we would need to have a sufficient amount of observations in each cell. So, let's check:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin |>\n  mutate(group = as_factor(group),\n         condition = as_factor(condition),\n         prototype_label = as_factor(prototype_label)) |>\n  count(group, condition, prototype_label, .drop = FALSE) |>\n  arrange(n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 4\n   group condition prototype_label     n\n   <fct> <fct>     <fct>           <int>\n 1 touch Atypical  dCoM2               0\n 2 touch Typical   dCoM2               0\n 3 touch Atypical  dCoM                9\n 4 click Atypical  dCoM2              11\n 5 click Typical   dCoM2              11\n 6 touch Typical   dCoM               14\n 7 touch Atypical  cCoM               21\n 8 touch Typical   cCoM               31\n 9 click Atypical  cCoM               32\n10 click Atypical  curved             36\n11 touch Atypical  curved             37\n12 click Typical   cCoM               48\n13 click Atypical  dCoM               50\n14 click Typical   dCoM               52\n15 touch Typical   curved             72\n16 click Typical   curved             84\n17 click Atypical  straight          189\n18 touch Atypical  straight          263\n19 click Typical   straight          494\n20 touch Typical   straight          598\n```\n:::\n:::\n\n\nSo, there are cells for which we have no observations at all. For simplicity, we therefore just lump all \"change of mind\"-type trajectories into one category:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin_prepped <-\n  dolphin |>\n  mutate(\n    prototype_label = case_when(\n     prototype_label %in% c('curved', 'straight') ~ prototype_label,\n     TRUE ~ 'CoM'\n    ),\n    prototype_label = factor(prototype_label,\n                             levels = c('straight', 'curved', 'CoM')))\n\ndolphin_prepped |>\n  select(RT, prototype_label)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,052 × 2\n      RT prototype_label\n   <dbl> <fct>          \n 1   950 straight       \n 2  1251 straight       \n 3   930 curved         \n 4   690 curved         \n 5   951 CoM            \n 6  1079 CoM            \n 7  1050 CoM            \n 8   830 straight       \n 9   700 straight       \n10   810 straight       \n# … with 2,042 more rows\n```\n:::\n:::\n\n\nHere is a plot of the data to be analyzed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin_prepped |>\n  ggplot(aes(x = log(RT), fill = condition)) +\n  geom_density(alpha = 0.4) +\n  facet_grid(group ~ prototype_label)\n```\n\n::: {.cell-output-display}\n![](02c-catPreds-execises_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Exercise 1\n\n::: callout-caution\n**Exercise 1a**\n\nUse `brm()` to run a linear regression model for the data set `dolphin_prepped` and the formula:\n\n`log RT ~ group * condition * prototype_label`\n\nSet the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\nfit <- brm(\n  formula = log(RT) ~ group * condition * prototype_label,\n  prior   = prior(student_t(1, 0, 3), class = \"b\"),\n  data    = dolphin_prepped\n  )\n```\n:::\n\n\n::: callout-caution\n**Exercise 1b**\n\nPlot the posteriors for population-level slope coefficients using the `tidybayes` package in order to:\n\n1.  determine which combination of factor levels is the default cell\n2.  check which coefficients have 95% CIs that do *not* include zero\n3.  try to use this latter information to address any of our research hypotheses (stated above)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\ntidybayes::summarise_draws(fit)\ntidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |>\n  filter(.variable != \"b_Intercept\") |>\n  ggplot(aes(y = .variable, x = .value)) +\n  tidybayes::stat_halfeye() +\n  labs(x = \"\", y = \"\") +\n  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n               lty = \"dashed\")\n\n# the default cell is for click-atypical-straight\n\n# coeffiencents with 95% CIs that do not include zero are:\n#   grouptouch, conditionTypical, prototype_labelCoM\n\n# none of these give us direct information about our research hypotheses\n```\n:::\n\n\n::: callout-caution\n**Exercise 1c**\n\nUse the `faintr` package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# 1. Typical trials are faster than atypical ones.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = condition == 'Typical',\n  higher = condition == 'Atypical'\n)\n\n# 2. CoM trials are slower than the other kinds of trials\n#    (straight and curved) together, and respectively.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label != 'CoM',\n  higher = prototype_label == 'CoM'\n)\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'CoM'\n)\n\n# 3. 'straight' trials are faster than 'curved' trials.\n# -> There is no evidence for this hypothesis\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'curved'\n)\n\n# 4. Click trials are slower than touch trials.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = group == 'touch',\n  higher = group == 'click'\n)\n```\n:::\n\n\n# Regression w/ metric & categorical predictors\n\n# Exercise 2\n\n::: callout-caution\n**Exercise 2a**\n\nCreate a new dataframe that contains only the mean values of the RT, and MAD for each animal (`exemplar`) and for correct and incorrect responses. Print out the `head` of the new dataframe.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# aggregate\ndolphin_agg <- dolphin |> \n  group_by(exemplar, correct) |> \n  dplyr::summarize(MAD = mean(MAD, na.rm = TRUE),\n                   RT = mean(RT, na.rm = TRUE))\n  \n# let's have a look\nhead(dolphin_agg)\n```\n:::\n\n\n::: callout-caution\n**Exercise 2b**\n\nRun a linear regression using brms. `MAD` is the dependent variable (i.e. the measure) and both `RT` and `correct` are independent variables (`MAD ~ RT + correct`). Tip: the coefficients might be really really small, so make sure the output is printed with enough numbers after the comma.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# specify the model \nmodel2 = brm(\n  # model formula\n  MAD ~ RT + correct, \n  # data\n  data = dolphin_agg\n  )\n\nprint(summary(model2), digits = 5)\n```\n:::\n\n\nTry to understand the coefficient table. There is one coefficient for `RT` and one coefficient for `correct` which gives you the change in MAD from incorrect to correct responses.\n\n::: callout-caution\n**Exercise 2c**\n\nPlot a scatter plot of MAD \\~ RT and color code it for correct responses (Tip: Make sure that `correct` is treated as a factor and not a numeric vector). Draw two predicted lines into the scatterplot. One for correct responses (\"lightblue\") and one for incorrect responses (\"orange\").\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\ndolphin_agg$correct <- as.factor(as.character(dolphin_agg$correct))\n\n# extract model parameters:\nmodel_intercept <- summary(model2)$fixed[1,1]\nmodel_RT <- summary(model2)$fixed[2,1]\nmodel_correct <- summary(model2)$fixed[3,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD,\n           color = correct)) + \n  geom_abline(intercept = model_intercept, slope = model_RT, color = \"orange\", size  = 2) +\n  geom_abline(intercept = model_intercept + model_correct , slope = model_RT, color = \"lightblue\",size  = 2) +\n  geom_point(size = 3, alpha = 0.3)\n```\n:::\n\n\n::: callout-caution\n**Exercise 2d**\n\nExtract the posteriors for the coefficients of both `RT` and `correct` from the model output (use the `spread_draws()` function), calculate their means and a 67% Credible Interval. Print out the `head` of the aggregated dataframe.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# get posteriors for the relevant coefficients\nposteriors2 <- model2 |>\n  spread_draws(b_RT, b_correct) |>\n  select(b_RT, b_correct) |> \n  gather(key = \"parameter\", value = \"posterior\")\n\n# aggregate\nposteriors2_agg <- posteriors2 |> \n  group_by(parameter) |> \n  summarise(mean_posterior = mean(posterior),\n            `67lowerCrI` = HDInterval::hdi(posterior, credMass = 0.67)[1],\n            `67higherCrI` = HDInterval::hdi(posterior, credMass = 0.67)[2]\n            )\n\n# print out\nposteriors2_agg\n```\n:::\n\n\n::: callout-caution\n**Exercise 2e**\n\nPlot the scatterplot from 2c and plot 50 sample tuples for the regression lines for correct and incorrect responses.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# sample 50 random numbers from 4000 samples\nrandom_50 <- sample(1:4000, 50, replace = FALSE)\n  \n# wrangle data frame\nposteriors3 <- model2 |>\n  spread_draws(b_Intercept, b_RT, b_correct) |>\n  select(b_Intercept, b_RT, b_correct) |> \n  # filter by the row numbers in random_50\n  slice(random_50)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD, \n           color = correct)) + \n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept, slope = b_RT), \n              color = \"orange\", size  = 0.1) +\n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept + b_correct, slope = b_RT), \n              color = \"lightblue\", size  = 0.1) +\n  geom_point(size = 3, alpha = 0.3)\n```\n:::\n\n\n::: callout-caution\n**Exercise 2f**\n\nGiven our model and our data, calculate the evidential ratio of correct responses exhibiting larger MADs than incorrect responses.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\nhypothesis(model2, 'correct > 0')\n```\n:::\n\n\n# Exercise 3\n\nHere is an aggregated data set `dolphin_agg` for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# aggregate\ndolphin_agg <- dolphin %>% \n  group_by(group, exemplar) %>% \n  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),\n                   RT = median(RT, na.rm = TRUE)) %>% \n  mutate(log_RT = log(RT))\n```\n:::\n\n\n::: callout-caution\n**Exercise 3a**\n\nStandardize (\"z-transform\") `log_RT` such that the mean is at zero and 1 unit corresponds to the standard deviation. Name it `log_RT_s`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\ndolphin_agg$log_RT_s <- scale(dolphin_agg$log_RT, scale = TRUE)\n```\n:::\n\n\n::: callout-caution\n**Exercise 3b**\n\nRun a linear model with `brms` that predicts `MAD` based on `log_RT_s`, `group`, and their two-way interaction.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\nmodel1 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg\n  )\n```\n:::\n\n\n::: callout-caution\n**Exercise 3c**\n\nPlot `MAD` (y) against `log_RT_s` (x) in a scatter plot and color-code for `group`. Plot the regression lines for the click and the touch group into the plot and don't forget to take possible interactions into account.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# extract posterior means for model coefficients\nIntercept = summary(model1)$fixed[1,1]\nlog_RT = summary(model1)$fixed[2,1]\ngroup = summary(model1)$fixed[3,1]\ninteraction = summary(model1)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[2], size = 2) \n```\n:::\n\n\n::: callout-caution\n**Exercise 3d**\n\nSpecify very skeptic priors for all three coefficients. Use a normal distribution with mean = 0, and sd = 10. Rerun the model with those priors.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# specify priors\npriors_model2 <- c(\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"grouptouch\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s:grouptouch\")\n)\n\n# model\nmodel2 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg,\n  prior = priors_model2\n  )\n```\n:::\n\n\n::: callout-caution\n**Exercise 3e**\n\nCompare the model output of model1 to model2. What are the differences and what are the reasons for these differences?\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# We can compare the model predictions by looking at the coefficients / plotting them:\nsummary(model1)\nsummary(model2)\n\n# extract posterior means for model coefficients\nIntercept = summary(model2)$fixed[1,1]\nlog_RT = summary(model2)$fixed[2,1]\ngroup = summary(model2)$fixed[3,1]\ninteraction = summary(model2)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[1], size = 2) \n\n# ANSWER: the magnitude of the coefficients is much smaller in model2, with the interaction term being close to zero. As a result, the lines in the plot are closer together and run in parallel. The reason for this change lies in the priors. We defined the priors of model2 rather narrowly, down weighing  data points larger or smaller than zero. This is a case of the prior dominating the posterior.\n```\n:::\n",
    "supporting": [
      "02c-catPreds-execises_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}