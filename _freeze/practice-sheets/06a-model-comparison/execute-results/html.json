{
  "hash": "466fd8f7ce7735dc672cc9a121bed751",
  "result": {
    "markdown": "---\ntitle: \"Bayesian regression: theory & practice\"\nsubtitle: \"06: Model comparison\"\nauthor: \"Michael Franke\"\nformat: html\neditor: visual\nexecute:\n  error: false\n  warning: false\n  message: false\ncallout-appearance: simple\n---\n\n\nLoad relevant packages and \"set the scene.\"\n\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin <- aida::data_MT\nrerun_models = FALSE\n```\n:::\n\n\n# Comparing models with LOO-CV and Bayes factors\n\nSuppose that the ground truth is a robust regression model generating our data (a robust regression uses a Student-t distribution as likelihood function):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1970)\n\n# number of observations\nN <- 100\n# 100 samples from a standard normal\nx <- rnorm(N, 0, 1)\n\nintercept <- 2\nslope <- 4\n\n# robust regression with a Student's t error distribution\n# with 1 degree of freedom\ny <- rt(N, df = 1, ncp = slope * x + intercept)\n\ndata_robust <- tibble(x = x, y = y)\n```\n:::\n\n\nA plot of the data shows that we have quite a few \"outliers\":\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(x,y) + \n  geom_smooth(color = project_colors[1], method = \"lm\") +\n  geom_point(color = project_colors[2], size = 2, alpha = 0.8)\n```\n\n::: {.cell-output-display}\n![](06a-model-comparison_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWe are going to compare two models for this data, a normal regression model and a robust regression model.\n\n## Normal and robust regression models\n\nA normal regression model uses a normal error function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_n <- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n)\n```\n:::\n\n\nWe will want to compare this normal regression model with a **robust regression model**, which uses a Student's t distribution instead as the error function around the linear predictor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_r <- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n  family = student()\n)\n```\n:::\n\n\nLet's look at the posterior inferences of both models about the true (known) parameters of the regression line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprep_summary <- function(fit, model) {\n  tidybayes::summarise_draws(fit) |> \n    mutate(model = model) |> \n    select(model, variable, q5, mean, q95) |> \n    filter(grepl(variable, pattern = '^b'))  \n}\n\nrbind(prep_summary(fit_n, \"normal\"), prep_summary(fit_r, \"robust\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 Ã— 5\n  model  variable       q5  mean   q95\n  <chr>  <chr>       <dbl> <dbl> <dbl>\n1 normal b_Intercept  2.56  7.69 13.1 \n2 normal b_x          7.31 13.5  19.6 \n3 robust b_Intercept  1.81  2.49  3.24\n4 robust b_x          4.99  6.09  7.24\n```\n:::\n:::\n\n\nRemember that the true intercept is 2 and the true slope is 4. Clearly the robust regression model has recovered the ground-truth parameters much better.\n\n## Leave-one-out cross validation\n\nWe can use the `loo` package to compare these two models based on their posterior predictive fit. Here's how:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_comp <- loo_compare(list(normal = loo(fit_n), robust = loo(fit_r)))\nloo_comp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       elpd_diff se_diff\nrobust    0.0       0.0 \nnormal -130.9      25.6 \n```\n:::\n:::\n\n\nWe see that the robust regression model is better by ca. -131 points of expected log predictive density. The table shown above is ordered with the \"best\" model on top. The column `elpd_diff` lists the difference in ELPD of every model to the \"best\" one. In our case, th estimated ELPD difference has a standard error of about 26. Computing a $p$-value for this using Lambert's $z$-score method, we find that this difference is \"significant\" (for which we will use other terms like \"noteworthy\" or \"substantial\" in the following):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nWe conclude from this that the robust regression model is much better at predicting the data (from a posterior point of view).\n\n## Bayes factor model comparison (with bridge sampling)\n\nWe use bridge sampling, as implemented in the formidable `bridgesampling` package, to estimate the (log) marginal likelihood of each model. To do this, we need also samples from the prior. To do this reliably, we need many more samples than we would normally need for posterior inference. We can `update()` existing fitted models, so that we do not have to copy-paste all specifications (formula, data, prior, ...) each time. It's important for `bridge_sampler()` to work that we save all parameters (including prior samples).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (rerun_models) {\n  # refit normal model\n  fit_n_4Bridge <- update(\n    fit_n,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  # refit robust model\n  fit_r_4Bridge <- update(\n    fit_r,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  normal_bridge <- bridge_sampler(fit_n_4Bridge, silent = T)\n  write_rds(normal_bridge, \"06-normal_bridge.rds\")\n  robust_bridge <- bridge_sampler(fit_r_4Bridge, silent = T)  \n  write_rds(robust_bridge, \"06-robust_bridge.rds\")\n} else {\n  normal_bridge <- read_rds(\"06-normal_bridge.rds\")  \n  robust_bridge <- read_rds(\"06-robust_bridge.rds\")\n}\n\nbf_bridge <- bridgesampling::bf(robust_bridge, normal_bridge)\n```\n:::\n\n\nWe can then use the `bf` (Bayes factor) method from the `bridgesampling` package to get the Bayes factor (here: in favor of the robust regression model):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbf_bridge\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimated Bayes factor in favor of robust_bridge over normal_bridge: 41136407426471809154394622543225576928422395904.00000\n```\n:::\n:::\n\n\nAs you can see, this is a very clear result. If we had equal levels of credence in both models, after seeing the data, our degree of belief in the robust regression model should ... well, virtually infinitely higer than our degree of belief in the normal model.\n\n\n# Comparing LOO-CV and Bayes factors\n\nLOO-CV and Bayes factor gave similar results in the Walkthrough. The results are qualitatively the same: the (true) robust regression model is preferred over the (false) normal regression model. Both methods give quantitative results, too. But here only the Bayes factor results have a clear intuitive interpretation. In this exercise we will explore the main conceptual difference between LOO-CV and Bayes factors, which is:\n\n-   LOO-CV compares models from a data-informed, *ex post* point of view based on a (repeatedly computed) **posterior predictive distribution**\n-   Bayes factor model comparison takes a data-blind, *ex ante* point of view based on the **prior predictive distribution**\n\nWhat does that mean in practice? -- To see the crucial difference, imagine that you have tons of data, so much that they completely trump your prior. LOO-CV can use this data to emancipate itself from any wrong or too uninformative prior structure. Bayes factor comparison cannot. If a Bayesian model is a likelihood function AND a prior, Bayes factors give the genuine Bayesian comparison, taking the prior into account. That is what you want when your prior structure are really part of your theoretical commitment. If you are looking for prediction based on weak priors AND a ton of data to train on, you should not use Bayes factors.\n\nTo see the influence of priors on model comparison, we are going to look at a very simple data set generated from a standard normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of observations\nN <- 100\n# data from a standard normal\ny <- rnorm(N)\n# list of data for Stan\ndata_normal <- tibble(y = y)\n```\n:::\n\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 1a\n\nUse `brms` to implement two models for inferring a Gaussian distribution.\n\n-   The first one has narrow priors for its parameters (`mu` and `sigma`), namely a Student's $t$ distribution with $\\nu = 1$, $\\mu = 0$ and $\\sigma = 10$.\n-   The second one has wide priors for its parameters (`mu` and `sigma`), namely a Student's $t$ distribution with $\\nu = 1$, $\\mu = 0$ and $\\sigma = 1000$.\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_narrow <- brm(\n  formula = y ~ 1,\n  data = data_normal,\n  prior = c(prior(\"student_t(1,0,10)\", class = \"Intercept\"),\n            prior(\"student_t(1,0,10)\", class = \"sigma\"))\n)\nfit_wide <- brm(\n  formula = y ~ 1,\n  data = data_normal,\n  prior = c(prior(\"student_t(1,0,1000)\", class = \"Intercept\"),\n            prior(\"student_t(1,0,1000)\", class = \"sigma\"))\n)\n```\n:::\n\n:::\n\n:::\n\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 1b\n\nCompare the models with LOO-CV, using the `loo` package, and interpret the outcome.\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_compare(\n  list(\n    wide   = loo(fit_wide),\n    narrow = loo(fit_narrow)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       elpd_diff se_diff\nwide   0.0       0.0    \nnarrow 0.0       0.1    \n```\n:::\n:::\n\n\nThe models are pretty much incomparable (equally good/bad) based on the LOO-CV.\n:::\n:::\n\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 1c\n\nUse the `bridgesampling` package to find an (approximate) Bayes factor for this model comparison.\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (rerun_models) {\n  fit_narrow_4Bridge <- update(\n    object = fit_narrow,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  \n  fit_wide_4Bridge <- update(\n    object = fit_wide,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  \n  narrow_bridge <- bridge_sampler(fit_narrow_4Bridge, silent = T)\n  write_rds(narrow_bridge, \"06-narrow_bridge.rds\")\n  \n  wide_bridge   <- bridge_sampler(fit_wide_4Bridge, silent = T)\n  write_rds(wide_bridge, \"06-wide_bridge.rds\")\n  \n} else {\n  narrow_bridge <- read_rds(\"06-narrow_bridge.rds\")  \n  wide_bridge <- read_rds(\"06-wide_bridge.rds\")\n}\n\nbridgesampling::bf(narrow_bridge, wide_bridge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimated Bayes factor in favor of narrow_bridge over wide_bridge: 9868.34657\n```\n:::\n:::\n\n\nThe Bayes factors in favor of the narrow model is about 10000. \nThat's massive evidence that, from a prior point of view, the narrow model is much better.\n:::\n:::\n\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 1d\n\nIf all went well, you should have seen a difference between the LOO-based and the BF-based model comparison. Explain what's going on in your own words.\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\nSince BF-based comparison looks at the models from the prior point of view, the model with wide priors is less precise, puts prior weight on a lot of \"bad\" paramter values and so achieves a very weak prior predicitive fit.\n\nThe LOO-based estimates are identical because both models have rather flexible, not too strong priors, and so the data is able to produce roughly the same posteriors in both models.\n\n\n:::\n:::\n\n# Comparing (hierarchical) regression models\n\nWe are going to consider an example from the mouse-tracking data. We use categorical variables `group` and `condition` to predict `MAD` measures. We are going to compare different models, including models which only differ with respect to random effects.\n\nLet's have a look at the data first to remind ourselves:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# aggregate\ndolphin <- dolphin %>% \n  filter(correct == 1) \n\n# plotting the data\nggplot(data = dolphin, \n       aes(x = MAD, \n           color = condition, fill = condition)) + \n  geom_density(alpha = 0.3, linewidth = 0.4, trim = F) +\n  facet_grid(~group) +\n  xlab(\"MAD\")\n```\n\n::: {.cell-output-display}\n![](06a-model-comparison_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 2a\n\nSet up four regression models and run them via `brms`:\n\n1.  Store in variable `model1_noInnteraction_FE` a regression with `MAD` as dependent variable, and as explanatory variables `group` and `condition` (but NOT the interaction between these two).\n2.  Store in variable `model2_interaction_FE` a regression with `MAD` as dependent variable, and as explanatory variables `group`, `condition` and the interaction between these two.\n3.  Store in variable `model3_interaction_RandSlopes` a model like `model2_interaction_FE` but also adding additionally random effects, namely random intercepts for factor `subject_id`.\n4.  Store in `model4_interaction_MaxRE` a model like `model2_interaction_FE` but with the maximal random effects structure licensed by the design of the experiment.\n\n::: {.callout-tip collapse=\"true\"}\n\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1_NOinteraction_FE = brm(\n  MAD ~ condition + group, \n  data = dolphin\n) \n\nmodel2_interaction_FE = brm(\n  MAD ~ condition * group, \n  data = dolphin\n)\n\nmodel3_interaction_RandSlopes = brm(\n  MAD ~ condition * group + (1 | subject_id), \n  data = dolphin\n) \n\nmodel4_interaction_MaxRE = brm(\n  MAD ~ condition * group + (1 + group | exemplar) + (1 | subject_id), \n  data = dolphin\n) \n```\n:::\n\n\n:::\n:::\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 2b\n\nThis exercise and the next are meant to have you think more deeply about the relation (or unrelatedness) of posterior inference and model comparison. Remember that, conceptually, these are two really different things.\n\nTo begin with, look at the summary of posterior estimates for model `model2_interaction_FE`. Based on these results, what would you expect: is the inclusion of the interaction term relevant for loo-based model comparison? In other words, do you think that `model2_interaction_FE` is better, equal or worse than `model2_NOinteraction_FE` under loo-based model comparison? Explain your answer.\n\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2_interaction_FE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ condition * group \n   Data: dolphin (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                     278.44     15.91   246.84   309.80 1.00     2167\nconditionTypical             -136.02     18.49  -172.01   -99.05 1.00     2108\ngrouptouch                   -204.09     22.04  -246.86  -159.39 1.00     2015\nconditionTypical:grouptouch   112.36     26.08    60.90   162.18 1.00     1876\n                            Tail_ESS\nIntercept                       2677\nconditionTypical                2279\ngrouptouch                      2202\nconditionTypical:grouptouch     2116\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   267.28      4.40   258.70   276.42 1.00     3326     2706\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nThe coefficient for the interaction term is credibly different from zero,\nin fact quite large. We would therefore expect that the data \"needs\" the\ninteraction term; a model without it is likely to fare worse.\n\n:::\n:::\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 2c\n\nNow compare the models directly using `loo_compare`. Compute the $p$-value (following Lambert) and draw conclusion about which, if any, of the two models is notably favored by LOO model comparison.\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_comp <- loo_compare(\n  loo(model1_NOinteraction_FE),\n  loo(model2_interaction_FE)\n)\nloo_comp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        elpd_diff se_diff\nmodel2_interaction_FE    0.0       0.0   \nmodel1_NOinteraction_FE -7.7       4.7   \n```\n:::\n:::\n\n\nThe model `model2_NOinteraction_FE` is worse by -7.7100632 points of expected log predictive density with a standard error of ca. 4.7363259. This translates into a \"significant\" difference, leading to the conclusion that the model with interaction term is really better:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.001470984\n```\n:::\n:::\n\n\n:::\n:::\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 3d\n\nNow, let's also compare models that differ only in their random effects structure. We start by looking at the posterior summaries for `model4_interaction_MaxRE`. Just by looking at the estimated coefficients for the random effects (standard deviations), would you conclude that these variables are important (e.g., that the data provides support for these parameters to be non-negligible)?\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4_interaction_MaxRE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ condition * group + (1 + group | exemplar) + (1 | subject_id) \n   Data: dolphin (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~exemplar (Number of levels: 19) \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                37.23     15.15     7.48    69.54 1.01      903\nsd(grouptouch)               26.84     16.49     1.59    63.79 1.00     1237\ncor(Intercept,grouptouch)    -0.68      0.42    -1.00     0.65 1.00     1960\n                          Tail_ESS\nsd(Intercept)                  861\nsd(grouptouch)                1844\ncor(Intercept,grouptouch)     2358\n\n~subject_id (Number of levels: 108) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)    77.34      8.79    60.61    95.76 1.00     1700     2403\n\nPopulation-Level Effects: \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                     280.28     24.28   232.11   327.04 1.00     2531\nconditionTypical             -138.09     26.19  -189.59   -86.39 1.00     2784\ngrouptouch                   -202.74     27.93  -257.49  -148.09 1.00     2484\nconditionTypical:grouptouch   111.79     28.46    55.46   166.45 1.00     3271\n                            Tail_ESS\nIntercept                       2092\nconditionTypical                2438\ngrouptouch                      2729\nconditionTypical:grouptouch     2602\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   254.82      4.31   246.43   263.41 1.00     7094     2745\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nAll the parameters from the RE structure, except for the correlation\nterm, are credibly bigger than zero, suggesting that the data lends\ncredence to these factors playing a role in the data-generating process.\n\n:::\n:::\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 3e\n\nCompare the models `model3_interaction_RandSlopes` and `model4_interaction_MaxRE` with LOO-CV. Compute Lambert's $p$-value and draw conclusions about which, if any, of these models is to be preferred by LOO-CV. Also, comment on the results from 3.b through 3.e in comparison: are the results the same, comparable, different ... ; and why so?\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_comp <- loo_compare(\n  loo(model4_interaction_MaxRE),\n  loo(model3_interaction_RandSlopes)\n)\n\nloo_comp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              elpd_diff se_diff\nmodel4_interaction_MaxRE       0.0       0.0   \nmodel3_interaction_RandSlopes -2.1       3.9   \n```\n:::\n\n```{.r .cell-code}\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9656232\n```\n:::\n:::\n\n\nLOO-CV finds no noteworthy difference between these models.\nIn tendency, the smaller model is even better.\nThis is different from the previous case in 3.b/c which might\nhave suggested that if a parameter $\\theta$ is credibly different\nfrom 0, then we also prefer to have it in the model for predictive accuracy.\nBut this is not the case in the current case (3.d/e) where the smaller model is preferred.\nAn explanation for this could be that not all parameters are equal.\nThe fixed effect terms have more impact on the variability of\npredictions than random effect terms.\n\n:::\n:::\n\n\n::: {.callout-caution collapse=\"false\"}\n## Exercise 3f\n\nCompare all four models using LOO-CV with method `loo_compare` and interpret the outcome. Which model is, or which models are the best?\n\n::: {.callout-tip collapse=\"true\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_compare(\n  loo(model1_NOinteraction_FE),\n  loo(model2_interaction_FE),\n  loo(model3_interaction_RandSlopes),\n  loo(model4_interaction_MaxRE)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              elpd_diff se_diff\nmodel4_interaction_MaxRE        0.0       0.0  \nmodel3_interaction_RandSlopes  -2.1       3.9  \nmodel2_interaction_FE         -48.1      13.6  \nmodel1_NOinteraction_FE       -55.8      14.7  \n```\n:::\n\n```{.r .cell-code}\n# model 4 is best, followed by model 3. \n# As models 3 and 4 are incomparable by LOO-CV (from part 3.c), \n# we only need to check if model 4 is better than model 2, which is on third place:\n\nloo_compr <- loo_compare(\n  loo(model4_interaction_MaxRE),\n  loo(model2_interaction_FE)\n)\n\n# We find that this difference is substantial:\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9656232\n```\n:::\n:::\n\n\n\n:::\n:::\n\n\n",
    "supporting": [
      "06a-model-comparison_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}