{
  "hash": "105f077b8b1d437fe3677747c5e3948e",
  "result": {
    "markdown": "---\ntitle: \"Bayesian regression: theory & practice\"\nsubtitle: \"02: Simple linear regression & categorical predictors\"\nauthor: \"Michael Franke & Timo Roettger\"\nformat: html\neditor: visual\nexecute:\n  error: false\n  warning: false\n  message: false\ncallout-appearance: simple\n---\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin <- aida::data_MT\n```\n:::\n\n\n# Exercises: categorical predictors\n\nWe want to regress `log RT` against the full combination of categorical factors `group`, `condition`, and `prototype_label`.\n\n`log RT ~ group * condition * prototype_label`\n\nThe research hypotheses we would like to investigate are:\n\n1.  Typical trials are faster than atypical ones.\n2.  CoM trials are slower than the other kinds of trials (straight and curved) together, and respectively.\n3.  'straight' trials are faster than 'curved' trials.\n4.  Click trials are slower than touch trials.\n\nBut for this to work (without at least mildly informative priors), we would need to have a sufficient amount of observations in each cell. So, let's check:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin |> \n  mutate(group = as_factor(group),\n         condition = as_factor(condition),\n         prototype_label = as_factor(prototype_label)) |> \n  count(group, condition, prototype_label, .drop = FALSE) |> \n  arrange(n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 4\n   group condition prototype_label     n\n   <fct> <fct>     <fct>           <int>\n 1 touch Atypical  dCoM2               0\n 2 touch Typical   dCoM2               0\n 3 touch Atypical  dCoM                9\n 4 click Atypical  dCoM2              11\n 5 click Typical   dCoM2              11\n 6 touch Typical   dCoM               14\n 7 touch Atypical  cCoM               21\n 8 touch Typical   cCoM               31\n 9 click Atypical  cCoM               32\n10 click Atypical  curved             36\n11 touch Atypical  curved             37\n12 click Typical   cCoM               48\n13 click Atypical  dCoM               50\n14 click Typical   dCoM               52\n15 touch Typical   curved             72\n16 click Typical   curved             84\n17 click Atypical  straight          189\n18 touch Atypical  straight          263\n19 click Typical   straight          494\n20 touch Typical   straight          598\n```\n:::\n:::\n\n\nSo, there are cells for which we have no observations at all. For simplicity, we therefore just lump all \"change of mind\"-type trajectories into one category:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin_prepped <- \n  dolphin |> \n  mutate(\n    prototype_label = case_when(\n     prototype_label %in% c('curved', 'straight') ~ prototype_label,\n     TRUE ~ 'CoM'\n    ),\n    prototype_label = factor(prototype_label,\n                             levels = c('straight', 'curved', 'CoM')))\n\ndolphin_prepped |> \n  select(RT, prototype_label)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,052 × 2\n      RT prototype_label\n   <dbl> <fct>          \n 1   950 straight       \n 2  1251 straight       \n 3   930 curved         \n 4   690 curved         \n 5   951 CoM            \n 6  1079 CoM            \n 7  1050 CoM            \n 8   830 straight       \n 9   700 straight       \n10   810 straight       \n# … with 2,042 more rows\n```\n:::\n:::\n\n\nHere is a plot of the data to be analyzed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin_prepped |> \n  ggplot(aes(x = log(RT), fill = condition)) +\n  geom_density(alpha = 0.4) +\n  facet_grid(group ~ prototype_label)\n```\n\n::: {.cell-output-display}\n![](02d-linReg-multiple-catPredictors_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n::: callout-caution\n**Exercise 1a**\n\nUse `brm()` to run a linear regression model for the data set `dolphin_prepped` and the formula:\n\n`log RT ~ group * condition * prototype_label`\n\nSet the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\nfit <- brm(\n  formula = log(RT) ~ group * condition * prototype_label,\n  prior   = prior(student_t(1, 0, 3), class = \"b\"),\n  data    = dolphin_prepped\n  )\n```\n:::\n\n\n::: callout-caution\n**Exercise 1b**\n\nPlot the posteriors for population-level slope coefficients using the `tidybayes` package in order to:\n\n1.  determine which combination of factor levels is the default cell\n2.  check which coefficients have 95% CIs that do *not* include zero\n3.  try to use this latter information to address any of our research hypotheses (stated above)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\ntidybayes::summarise_draws(fit)\ntidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |> \n  filter(.variable != \"b_Intercept\") |> \n  ggplot(aes(y = .variable, x = .value)) +\n  tidybayes::stat_halfeye() +\n  labs(x = \"\", y = \"\") +\n  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n               lty = \"dashed\")\n\n# the default cell is for click-atypical-straight\n\n# coeffiencents with 95% CIs that do not include zero are:\n#   grouptouch, conditionTypical, prototype_labelCoM\n\n# none of these give us direct information about our research hypotheses\n```\n:::\n\n\n::: callout-caution\n**Exercise 1c**\n\nUse the `faintr` package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# 1. Typical trials are faster than atypical ones.\n# -> There is overwhelming evidence that this is true \n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = condition == 'Typical',\n  higher = condition == 'Atypical'\n)\n\n# 2. CoM trials are slower than the other kinds of trials \n#    (straight and curved) together, and respectively.\n# -> There is overwhelming evidence that this is true \n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label != 'CoM',\n  higher = prototype_label == 'CoM'\n)\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'CoM'\n)\n\n# 3. 'straight' trials are faster than 'curved' trials.\n# -> There is no evidence for this hypothesis\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'curved'\n)\n\n# 4. Click trials are slower than touch trials.\n# -> There is overwhelming evidence that this is true \n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = group == 'touch',\n  higher = group == 'click'\n)\n```\n:::\n\n\n## Exercises: metric and categorical predictors\n\nHere is an aggregated data set `dolphin_agg2` for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# aggregate\ndolphin_agg2 <- dolphin %>% \n  filter(correct == 1) %>% \n  group_by(exemplar, group, condition) %>% \n  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),\n                   RT = median(RT, na.rm = TRUE)) %>% \n  mutate(log_RT = log(RT))\n```\n:::\n\n\n::: callout-caution\n**Exercise 2a**\n\nRun a model predicting MAD based on *standardized* `log_RT`, `group`, `condition`, and *their three-way interaction*. Set a seed = 999.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# standardize\ndolphin_agg2$log_RT_s <- scale(dolphin_agg2$log_RT, scale = TRUE)\n\n# model\nmodel3 = brm(\n  MAD ~ log_RT_s * group * condition, \n  data = dolphin_agg2,\n  iter = 2000,\n  chains = 4,\n  seed = 999\n  )\n```\n:::\n\n\n::: callout-caution\n**Exercise 2b**\n\nLook at the output. Extract posterior means and 95% CrIs for the following predictor level combinations. One row corresponds to one concrete combination of levels. (Tip: check your results by plotting them against the data)\n\n-   Combination1: log_RT_s == 0; group == click; condition == Atypical\n-   Combination2: log_RT_s == 0; group == touch; condition == Atypical\n-   Combination3: log_RT_s == 1; group == touch; condition == Typical\n-   Combination4: log_RT_s == 2; group == touch; condition == Atypical\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\nposteriors3a <- model3 %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`,\n               `b_log_RT_s:grouptouch:conditionTypical`\n               ) %>% \n  mutate(Combination1 = b_Intercept + (0 * b_log_RT_s),\n         Combination2 = b_Intercept + (0 * b_log_RT_s) + b_grouptouch,\n         Combination3 = b_Intercept + (1 * b_log_RT_s) + b_grouptouch + \n           b_conditionTypical + `b_grouptouch:conditionTypical` + (1 * `b_log_RT_s:grouptouch`) + \n           (1 * `b_log_RT_s:conditionTypical`) + (1 * `b_log_RT_s:grouptouch:conditionTypical`),\n         Combination4 = b_Intercept + (2 * b_log_RT_s) + b_grouptouch + \n           (2 * `b_log_RT_s:grouptouch`)) %>% \n  dplyr::select(Combination1, Combination2,\n                Combination3, Combination4) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter) %>% \n  summarise(mean_posterior = mean(posterior),\n            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],\n            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])\n\nposteriors3a\n```\n:::\n\n\n::: callout-caution\n**Exercise 2c**\n\nDefine the following priors and run the model3 again:\n\n-   log_RT_s: student-t (df = 3, mean = 0, sd = 30)\n-   grouptouch: student-t (df = 3, mean = 100, sd = 200)\n-   conditionTypical: student-t (df = 3, mean = 0, sd = 200)\n-   log_RT_s:grouptouch: normal (mean = 0, sd = 30)\n-   log_RT_s:conditionTypical: normal (mean = 0, sd = 30)\n-   grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 200)\n-   log_RT_s:grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 30)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\npriors_model3 <- c(\n   set_prior(\"student_t(3,0,30)\", class = \"b\", coef = \"log_RT_s\"),\n   set_prior(\"student_t(3,100,200)\", class = \"b\", coef = \"grouptouch\"),\n   set_prior(\"student_t(3,0,200)\", class = \"b\", coef = \"conditionTypical\"),\n   set_prior(\"normal(0,30)\", class = \"b\", coef = \"log_RT_s:grouptouch\"),\n   set_prior(\"normal(0,30)\", class = \"b\", coef = \"log_RT_s:conditionTypical\"),\n   set_prior(\"student_t(3,0,200)\", class = \"b\", coef = \"grouptouch:conditionTypical\"),\n   set_prior(\"student_t(3,0,30)\", class = \"b\", coef = \"log_RT_s:grouptouch:conditionTypical\")\n)\n\n# model\nmodel3b = brm(\n  MAD ~ log_RT_s * group * condition, \n  data = dolphin_agg2,\n  iter = 2000,\n  chains = 4,\n  prior = priors_model3\n  )\n```\n:::\n\n\n::: callout-caution\n**Exercise 2d**\n\nCompare the two posterior estimates from model3 and model3b. What has changed?\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# extract posteriors for model2\nposteriors3a <- model3 %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  select(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter)\n\n# extract posteriors for model2b\nposteriors3b <- model3b %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  select(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter)\n\n# plot posteriors for model2\nggplot(posteriors3a, aes(x = posterior, y = parameter)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nMAD\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-250, 250))\n  \n# plot posteriors for model2b\nggplot(posteriors3b, aes(x = posterior, y = parameter)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nMAD\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-250, 250))\n\n# ANSWER:The model output does not change much. Overall, the posteriors are a little tighter and closer to zero for model3b\n```\n:::\n\n\n::: callout-caution\n**Exercise 2a**\n\nSuppose you have the following aggregated data set and want to run the following linear model: `AUC ~ condition`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# aggregate\ndolphin_agg3 <- dolphin %>% \n  filter(correct == 1) %>% \n  group_by(subject_id, condition) %>%\n  dplyr::summarize(AUC = median(AUC, na.rm = TRUE)) \n\ndolphin_agg3$AUC_s <- scale(dolphin_agg3$AUC, scale = TRUE)\n```\n:::\n\n\nDeviation code the effect of condition, such that the Intercept gives you the grand average of `AUC_s` and the coefficient for condition gives you the difference between Atypical + Typical exemplars. Check last week's reading of Bodo Winter's book again (or google).\n\nSpecify an agnostic prior for the effect of condition and run the model from above (set `seed = 333`).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\n# contrast code condition\nc <- contr.treatment(2)\n\n# divide by 2 for it to represent the difference\nmy.coding <- matrix(rep(1/2, 2), ncol = 1)\nmy.simple <- c - my.coding\n\n# make factor\ndolphin_agg3$condition <- as.factor(dolphin_agg3$condition)\n# associate with contrast\ncontrasts(dolphin_agg3$condition) = my.simple\n\npriors_agnostic <- c(\n   # Atypical < Typical\n   set_prior(\"normal(0, 3)\", class = \"b\", coef = \"condition2\")\n)\n\nmodel4 = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_agnostic\n  )\n```\n:::\n\n\n::: callout-caution\n**Exercise 2f**\n\nNow suppose you have three people who want to encode their subjective beliefs about whether and how group and condition affect `AUC_s`. To keep your solutions comparable, we assume prior beliefs are normally distributed and there are three types of beliefs:\n\n1.  A strong belief in a directional relationship: The person assumes that there is a difference between two conditions (A\\>B). The mean of the assumed differences is 3 units of AUC_s with a SD of 0.5.\n\n2.  An agnostic belief in a directional relationship: Both A\\>B and B\\>A are plausible, but uncertainty is high. The mean of the most plausible distribution is 0 with a SD of 3, i.e. a rather wide distribution, allowing effects in both directions.\n\nHere are three researchers and their prior beliefs:\n\n*Michael* holds strong prior beliefs that Typical exemplars exhibit less curvature than Atypical exemplars.\n\n*Nina* is agnostic about the effect of condition on `AUC_s`.\n\nAs opposed to Michael, *Jones* holds strong prior beliefs that Typical exemplars exhibit MORE curvature than Atypical exemplars.\n\nSpecify priors for Michael, Nina, and Jones, and run models (set seed = 323) for all of these scenarios. Look at the results (maybe plot the posteriors if that helps you) and briefly describe how the priors affected the posteriors.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show solution\"}\npriors_Michael <- c(\n   # Atypical > Typical\n   set_prior(\"normal(-3, 0.5)\", class = \"b\", coef = \"condition2\")\n)\n\npriors_Nina <- c(\n   # Atypical < Typical\n   set_prior(\"normal(0, 3)\", class = \"b\", coef = \"condition2\")\n)\n\npriors_Jones <- c(\n      set_prior(\"normal(3, 0.5)\", class = \"b\", coef = \"condition2\")\n)\n\n\n# model\nmodel5_Michael = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Michael\n  )\n\nmodel5_Nina = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Nina\n  )\n\nmodel5_Jones = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Jones\n  )\n\n# extract posteriors\n\n## Michael\nposteriors_Michael <- model5_Michael %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  mutate(model = \"Michael\")\n  \n\n## Nina\nposteriors_Nina <- model5_Nina %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\")  %>% \n  mutate(model = \"Nina\")\n\n## Jones\nposteriors_Jones <- model5_Jones %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  mutate(model = \"Jones\")\n\n# add to one df\nposteriors_all <- rbind(posteriors_Michael, posteriors_Nina, posteriors_Jones)\n\n# plot posteriors for all models\nggplot(posteriors_all, aes(x = posterior, y = parameter, fill = model, color = model)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95, alpha = 0.4) +\n    facet_grid(model~ .) +\n    # add axes titles\n    xlab(\"\\nAUC\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf, color = \"black\",\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-1.5, 0.5))\n\n#ANSWER: The agnostic Nina serves as a baseline. With a weakly informative prior, the likelihood dominates the posterior.\n# Michael has a strong preconception that Atypical exemplars elicit larger AUC values and in comparison to Jones, the posterior distribution is slightly shifted more toward negative values.\n# Jones has a strong preconception that Typical exemplars elicit larger AUC values and in comparison to Jones and Michael, the posterior distribution is slightly shifted AWAY from negative values.\n# Here, the priors, although encoding strong prior beliefs, have only little impact on the posteriors but do shift posteriors toward the priors.\n```\n:::\n",
    "supporting": [
      "02d-linReg-multiple-catPredictors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}