{
  "hash": "840aeeb2242f4db10073429823c5efc6",
  "result": {
    "markdown": "---\ntitle: \"Bayesian regression: theory & practice\"\nsubtitle: \"05a: Hierarchical regression models\"\nauthor: \"Michael Franke & Timo Roettger\"\nformat: html\neditor: visual\nexecute:\n  error: false\n  warning: false\n  message: false\ncallout-appearance: simple\n---\n\n\nLoad relevant packages and \"set the scene.\"\n\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndolphin <- aida::data_MT\nmy_scale <- function(x) c(scale(x))\n```\n:::\n\n\n## Introduction\n\nThe main **learning goals** of this week's practical parts are:\n\n- learning how to implement multilevel linear models with `brms` including\n- random intercept models\n- random slope models\n\n## The independence assumption\n\nLast week, we ended on a conundrum. We looked at the probability of observing a straight trajectory predicted by response latency. Here is the plot for all data in the click group plus a logistic smooth term:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up data frame\ndolphin_agg <- dolphin %>% \n  filter(correct == 1,\n         group == \"click\") %>% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\ndolphin_agg$straight_numeric <- as.numeric(as.character(dolphin_agg$straight))\n\n# plot predicted values against data\nggplot(data = dolphin_agg,\n       aes(x = log_RT_s, y = straight_numeric)) +\n  geom_point(position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_smooth(method = \"glm\", color = \"red\",\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ggtitle(\"overall relationship\") +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThis picture suggest a negative relationship between the probability of observing straight trajectories (y) and peoples' response times (x) (i.e. line goes down). \n\nBut this analysis looked at all responses at once and disregarded that responses came from groups of sources. For example, responses that come from one and the same participant are dependent on each other because participants (`subject_id`) might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. Another group of data points is related to different stimuli (`exemplars`). Different stimuli might have some inherent properties that lead to different response times and different proportions of straight trajectories. So analyzing the data without telling the model about these groups violates an important assumption of linear models. The **independence assumption**.\n\nLet's look at these groups individually, starting by aggregating over over `subject_id`s and `exemplar`s and plot the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# aggregate over subjects\ndolphin_agg2 <- dolphin_agg %>% \n  group_by(subject_id) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for subjects\nggplot(data = dolphin_agg2,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  # we use the geom_smooth function here as a rough proxy of the relationship \n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = \"red\",\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"subject aggregates\") +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nHuh. That is interesting. So if we aggregate over subjects, i.e. each data point is one subject reacting to all exemplars, we get a positive relationship between response latency and the proportion of straight trajectories. The slower the reaction the more likely a straight trajectory. That could mean that those participants that are generally slower are also the ones that tend to move more often in a straight fashion. It also makes sense to some extent. Maybe those participants seem to wait until they have made their decision and then move to the target immediately, while other participants move upwards right away and make their decision on the fly during the decision. \n\nNow, let's aggregate over `exemplar`s:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# aggregate over exemplars\ndolphin_agg3 <- dolphin_agg %>% \n  group_by(exemplar) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for exemplars\nggplot(data = dolphin_agg3,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = \"red\",\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"stimuli aggregates\") +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nIf we look at the stimuli aggregates, i.e. each data point is one exemplar that all subjects have reacted to, we get a negative relationship between response latency and the proportion of straight trajectories. The quicker the reaction the more likely a straight trajectory. This could potentially reflect the difficulty of the categorization task. Maybe those exemplars that are inherently less ambiguous, for example the typical exemplars, don't exhibit any response competition and are thus faster and more often straight. \n\nUltimately, we use our models to make a generalizing statement about a population. If our theory predicts a relationship between straight trajectories and response latency (without further nuance), we should find this relationship across the population of people AND the population of stimuli. But if we say, \"there are more straight trajectories in faster responses\", this claim seems to be only true for within-participant behavior. So we need to inform our models about such groupings in our data, or we might overconfidently make predictions.\n\n## Multilevel models\n\nLet's assume the following simple model. This model assumes that all data points are independent. We know they are not. But bear with me. This model predicts the the probability of straight trajectories from the predictor `log_RT_s`. Predictors are also called **fixed effects**.\n\n\n::: {.cell warnings='false' messages='false' hash='05a-hierarchical-models_cache/html/unnamed-chunk-6_270b6e9077be5514c92a562539b561b3'}\n\n```{.r .cell-code}\nsimpl.mdl <- brm(straight ~ log_RT_s, \n               data = dolphin_agg,\n               family = \"bernoulli\",\n               seed = 99)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimpl.mdl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: straight ~ log_RT_s \n   Data: dolphin_agg (Number of observations: 942) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.80      0.07     0.66     0.94 1.00     3957     2759\nlog_RT_s     -0.22      0.07    -0.35    -0.09 1.00     3997     2886\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nWe get an intercept of 0.8 log-odds (corresponds to ~69%), i.e. the probability of a straight trajectory at `log_RT_s == 0`. With every unit of `log_RT_s` the log-odd value becomes smaller by 0.23. This negative slope has a 95% CrI of -0.36 to -0.09. Let's plot these results in a scatter plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract predicted values\npredicted_values <- simpl.mdl %>%\n  spread_draws(b_Intercept, b_log_RT_s) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-4, 6, 0.5))) %>% \n  unnest(log_RT) %>%\n  # transform into proportion space using the plogis function\n  mutate(pred = plogis(b_Intercept + b_log_RT_s*log_RT)) %>%\n  group_by(log_RT) %>%\n  summarise(pred_m = mean(pred, na.rm = TRUE),\n            pred_low = quantile(pred, prob = 0.025),\n            pred_high = quantile(pred, prob = 0.975)) \n\nggplot(data = predicted_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_agg,\n             position = position_jitter(height = 0.02), alpha = 0.2,\n             aes(x = log_RT_s, y = straight_numeric)) +\n  geom_ribbon(aes(ymin = pred_low, ymax = pred_high), alpha=0.2) +\n  geom_line(size = 1.5, color = \"red\") +\n  xlim(-3,6) +\n  ylab(\"probability of straight trajectory\") +  \n  labs(title = \"Model that ignores groupings\",\n       subtitle = \"Ribbon represents the 95% CrI\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nNotice that the Credible Interval is rather narrow here, suggesting much certainty around the effect of reaction time. Zero is not included in the CrI. Thus, we might make confident statements about the proposed relationship given the data, the priors, and the model.  \n\nWe know already, this model ignores important random sources of variability, like the sample of participants and stimuli. Since both participants and (to some extent) stimuli are sampled randomly from a population of people and stimuli, we often call these grouping levels *random effects*. Let's construct a model that accounts for the aforementioned groupings in the data.\n\nBecause these models *mix* fixed effects with random effects, these type of models are also called **mixed effects models**.\n\nHere is a model that allows varying intercepts for `subject_id` and `exemplar.` We add these random effects by the following notation: `(1 | GROUP)`.\n\n\n::: {.cell warnings='false' messages='false' hash='05a-hierarchical-models_cache/html/unnamed-chunk-9_0c076d9bd2db053f727f9dcfd33748d5'}\n\n```{.r .cell-code}\nrand.icpt <- brm(straight ~ log_RT_s +\n                       # specify varying intercept effects\n                       (1 | subject_id) + \n                       (1 | exemplar), \n               data = dolphin_agg, family = \"bernoulli\",\n               seed = 99)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrand.icpt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: straight ~ log_RT_s + (1 | subject_id) + (1 | exemplar) \n   Data: dolphin_agg (Number of observations: 942) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~exemplar (Number of levels: 19) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.42      0.14     0.18     0.72 1.00     1633     2116\n\n~subject_id (Number of levels: 53) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.86      0.14     0.60     1.16 1.00     1297     2151\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.93      0.18     0.59     1.29 1.00     2028     2490\nlog_RT_s     -0.50      0.11    -0.71    -0.29 1.00     3711     2849\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nThe model output has slightly changed. First, we now see \"Group-Level Effects\". These are our posterior estimates of how much `subject_id`s an `exemplar`s vary with regard to their log-odds for straight trajectories at `log_RT_s == 0` (i.e. their Intercepts). There is substantial variability across both exemplars and subjects, with subjects varying much more in their intercept than exemplars.\n\nWe also get our population-level effects. The intercept of 0.93 log-odds (corresponds to ~72%). With every unit of `log_RT_s` the log-odd value becomes smaller by 0.5. This negative slope has a 95% CrI of -0.71 to -0.30. \n\nSo as you can see, our inference already changed quite a bit in terms of the population-level estimates and the estimated (un)certainty arround these estimates.\n\nLet's plot the results in a scatter plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract predicted values\npredicted_values <- rand.icpt %>%\n  spread_draws(b_Intercept, b_log_RT_s) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-4, 6, 0.5))) %>% \n  unnest(log_RT) %>%\n  # transform into proportion space using the plogis function\n  mutate(pred = plogis(b_Intercept + b_log_RT_s*log_RT)) %>%\n  group_by(log_RT) %>%\n  summarise(pred_m = mean(pred, na.rm = TRUE),\n            pred_low = quantile(pred, prob = 0.025),\n            pred_high = quantile(pred, prob = 0.975)) \n\nggplot(data = predicted_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_agg,\n             position = position_jitter(height = 0.02), alpha = 0.2,\n             aes(x = log_RT_s, y = straight_numeric)) +\n  geom_ribbon(aes(ymin = pred_low, ymax = pred_high), alpha=0.2) +\n  geom_line(size = 1.5, color = \"red\") +\n  xlim(-3,6) +\n  ylab(\"probability of straight trajectory\") +\n  labs(title = \"Model with varying intercepts\",\n       subtitle = \"Ribbon represents the 95% CrI\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nComparing the plot to the one from above, we get a very similar picture, but our model already suggest a little bit more uncertainty about our estimate. Even if we allow for varying intercepts, we still get a positive relationship though. \n\nWe can now inspect the random effect estimates for individual `subject_id`s and `exemplar`s: We get the posterior estimates ( + SEs, 95% CrIs) for all levels of the grouping variables. The estimates represent the difference between the overall population level estimate and that of the specific group levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranef(rand.icpt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$exemplar\n, , Intercept\n\n               Estimate Est.Error        Q2.5      Q97.5\nalligator    0.29664065 0.2839397 -0.23666395  0.8922621\nbat         -0.07875892 0.2791318 -0.62483335  0.4759791\nbutterfly   -0.20711951 0.2565282 -0.74147113  0.2719468\ncat          0.02422052 0.2638518 -0.50573179  0.5647132\nchameleon    0.25285925 0.2796560 -0.25605859  0.8329672\ndog         -0.20748745 0.2778551 -0.76478923  0.3307190\neel          0.21959741 0.2708082 -0.29360463  0.7731756\ngoldfish     0.67690698 0.3583769  0.07021097  1.4542087\nhawk        -0.19354822 0.2632340 -0.72949060  0.2950843\nhorse       -0.06547946 0.2642889 -0.60051776  0.4539291\nlion        -0.21745188 0.2658150 -0.76532219  0.2798489\npenguin     -0.12082989 0.2626580 -0.65678444  0.3705791\nrabbit       0.02857169 0.2652022 -0.49630990  0.5598899\nrattlesnake  0.39565175 0.3132874 -0.15515478  1.0642429\nsalmon       0.07812438 0.2723454 -0.45868582  0.6357207\nsealion     -0.10022600 0.2646919 -0.63935881  0.4214356\nshark        0.18408018 0.2732332 -0.33224680  0.7430125\nsparrow     -0.36713899 0.2693456 -0.92275789  0.1220336\nwhale       -0.60139248 0.3189036 -1.27395324 -0.0312189\n\n\n$subject_id\n, , Intercept\n\n        Estimate Est.Error        Q2.5      Q97.5\n1002 -0.58217611 0.4331987 -1.42054088  0.2668537\n1003  1.45798552 0.6019999  0.38320589  2.7336501\n1007 -0.41476972 0.4433469 -1.27337271  0.4682395\n1010 -1.03498753 0.4536692 -1.96507914 -0.1518360\n1013  0.29217577 0.4822838 -0.62620886  1.2299748\n1014 -1.77492873 0.5113083 -2.83053284 -0.8204765\n1015 -0.03291623 0.4673486 -0.93299326  0.9307480\n1019  0.24262152 0.4810149 -0.68693751  1.2122711\n1020 -0.16061943 0.4857228 -1.10572456  0.7836680\n1022 -0.37675377 0.4383506 -1.21811109  0.4810185\n1023  0.62500797 0.5896005 -0.44191542  1.8642831\n1026 -0.04654578 0.4735256 -0.92907339  0.8992235\n1028  0.11908178 0.4940747 -0.85992026  1.1146653\n1030 -0.64625123 0.4501035 -1.53546348  0.2545829\n1031  0.18554237 0.5246226 -0.77931545  1.2794429\n1033  0.02692954 0.4750046 -0.86840823  0.9844282\n1034 -0.31659189 0.4635749 -1.22548231  0.6250993\n1037 -0.37588288 0.4623166 -1.29064251  0.5124747\n1039  0.89325619 0.5226952 -0.07521275  1.9776468\n1041  0.58875289 0.5087906 -0.36116306  1.6320039\n1044 -0.98992111 0.4827241 -1.94991937 -0.0447794\n1045  0.57947315 0.5218448 -0.39045123  1.6435183\n1046 -0.56054974 0.4533999 -1.48346785  0.3097504\n1048  0.33809179 0.5316130 -0.68982138  1.4308715\n1049  0.65408468 0.4906679 -0.26560189  1.6566007\n1050  0.06734755 0.4621591 -0.79858035  1.0107753\n1055  0.61960467 0.5168919 -0.33511400  1.7329353\n1059 -0.32893792 0.4474631 -1.18663849  0.5299382\n1060  1.10882291 0.5871842  0.05861994  2.3347122\n1061 -0.43282887 0.4600217 -1.34525566  0.4416522\n1063 -0.71143900 0.4516846 -1.59286754  0.1555908\n1064 -0.98699013 0.4450768 -1.87585067 -0.1390347\n1066  0.32446997 0.4855996 -0.58264834  1.3260861\n1068 -0.15487116 0.4842649 -1.09292633  0.8323122\n1070  0.71217002 0.5131646 -0.26426401  1.7457481\n1071  0.16957943 0.4981586 -0.79135321  1.1986620\n1072 -0.09756132 0.4373826 -0.93830606  0.7867572\n1074  0.42577960 0.4742148 -0.46907019  1.3919983\n1076  0.69825574 0.5059274 -0.25619353  1.7360072\n1078 -0.42669016 0.4424631 -1.28378049  0.4311160\n1079 -0.30581173 0.4576289 -1.16816577  0.6259672\n1083 -1.36559377 0.4615392 -2.27839613 -0.4602387\n1084  0.24205767 0.4508348 -0.62457126  1.1837598\n1085  1.34190565 0.5645348  0.30314798  2.4986375\n1088  0.62539368 0.5173937 -0.35139551  1.6889025\n1091 -0.50741803 0.4382883 -1.38725215  0.3621790\n1095 -0.31386997 0.4651998 -1.18178839  0.6313295\n1096  0.87148610 0.5874917 -0.19660599  2.1022159\n1099 -1.36424554 0.5036560 -2.36866284 -0.4125652\n1101 -0.46018075 0.4575841 -1.32179781  0.4372096\n1104  0.35907094 0.5026519 -0.59211854  1.3718769\n1105  1.09765242 0.5306576  0.13869299  2.2162662\n1107  0.07738874 0.4758695 -0.85215735  1.0146667\n```\n:::\n:::\n\n\nNote how much people/stimuli actually differ. With that information we can compare the individual groups with the overall population estimates. Let's do that for `subject_id`s. A little bit of data wrangling is required (and I am sure there are much more elegant ways to do this).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the random effects for subject_ids\nrandom_matrix <- ranef(rand.icpt)$subject_id[, , \"Intercept\"] %>% \n  round(digits = 2)\n\n# make data frame\nrandom_df <- data.frame(subject_id = row.names(random_matrix), random_matrix) %>% \n  mutate(Intercept = round(fixef(rand.icpt)[1],2),\n         Slope = round(fixef(rand.icpt)[2],2),\n         adjusted_int = Estimate + Intercept) %>% \n  mutate(log_RT = list(seq(-4, 6, 0.5))) %>% \n  unnest(log_RT) %>%\n  mutate(pred_m = plogis(adjusted_int + Slope*log_RT))\n\n# plot the individual regression lines on top of the population estimate\nggplot(data = predicted_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_agg,\n             position = position_jitter(height = 0.02), alpha = 0.2,\n             aes(x = log_RT_s, y = straight_numeric)) +\n  geom_line(data = random_df, \n            aes(x = log_RT, y = pred_m, group = subject_id),\n            size = 0.5, alpha = 0.2) +\n  geom_line(size = 1.5, color = \"red\") +\n  xlim(-3,6) +\n  ylab(\"probability of straight trajectory\") +\n  labs(title = \"Model with varying intercepts\",\n       subtitle = \"Thin lines are model estimates for each subject\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nBut wait. It's not only the case that different people/stimuli might have different baselines. Different people/stimuli might actually differ in terms of the investigated relationship between probability between straight trajectories and response time. There might be some people who produce less curvy movements for faster responses. Moreover, the degree of the relationships might differ quite a bit across groupings. We can inform our model about this. So basically we allow the model not only to vary the intercepts for each group level but also how much they are affected by the predictor. We call these varying effects *random slopes*.  \n\nHere we will specify uncorrelated slopes by the notation `(PREDICTOR || GROUP)`. \nThis does not estimate possible correlations between intercepts and slopes. Keep that in mind, as this is another piece of information that you might want to estimate. We don't do that here to keep our models relatively slim and easy to fit. If we want to estimate this correlation as well, we write `(PREDICTOR | GROUP)`. \n\nLet's also specify some weakly informative priors to speed up sampling.\n\n(Note the sampling will take a couple of minutes or so. Each grouping level represents yet another parameter to estimate.)\n\n\n::: {.cell hash='05a-hierarchical-models_cache/html/unnamed-chunk-14_238ed573eadcb2b67e860912a3118dd7'}\n\n```{.r .cell-code}\npriors <- c(\n  #priors for all fixed effects (here only log_RT_s)\n  set_prior(\"student_t(3, 0, 3)\", class = \"b\"),\n  #prior for the Intercept\n  set_prior(\"student_t(3, 0, 3)\", class = \"Intercept\"),\n  #prior for all SDs including the varying intercepts and slopes for both groupings\n  set_prior(\"student_t(3, 0, 3)\", class = \"sd\")\n)\n\nrand.slopes <- brm(straight ~ log_RT_s +\n                       # these are the slopes\n                       (log_RT_s || subject_id) + \n                       (log_RT_s || exemplar), \n               data = dolphin_agg,\n               family = \"bernoulli\",\n               prior = priors,\n               seed = 99)\n\nrand.slopes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: straight ~ log_RT_s + (log_RT_s || subject_id) + (log_RT_s || exemplar) \n   Data: dolphin_agg (Number of observations: 942) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~exemplar (Number of levels: 19) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.44      0.15     0.16     0.75 1.00     1233     1127\nsd(log_RT_s)      0.20      0.14     0.01     0.51 1.00     1405     1906\n\n~subject_id (Number of levels: 53) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.80      0.16     0.52     1.14 1.00     1324     2220\nsd(log_RT_s)      0.59      0.19     0.24     0.98 1.00     1152     1499\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.88      0.17     0.54     1.23 1.00     1876     2309\nlog_RT_s     -0.55      0.17    -0.88    -0.23 1.00     2136     2331\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nWe now see two coefficients for each grouping variable. For both `subject_id` and `exemplar`, you get one coefficient for the intercept and one for the effect of `log_RT_s`. Again, you can see that `subject_id`s differ quite a bit regarding the effect of `log_RT_s`.\n\nNote also that the population level estimate has become much more uncertain with a wider CrI. In the varying-intercept model above, the posterior mean for `log_RT_s` was -0.5 [-0.71, -0.30]. Now it is -0.55 [-0.9, -0.24]. \n\nLet's extract the random effect (both intercepts and slopes) again and plot them into our graph. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract predicted values for population parameter\npredicted_values <- rand.slopes %>%\n  spread_draws(b_Intercept, b_log_RT_s) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-4, 6, 0.5))) %>% \n  unnest(log_RT) %>%\n  # transform into proportion space using the plogis function\n  mutate(pred = plogis(b_Intercept + b_log_RT_s*log_RT)) %>%\n  group_by(log_RT) %>%\n  summarise(pred_m = mean(pred, na.rm = TRUE),\n            pred_low = quantile(pred, prob = 0.025),\n            pred_high = quantile(pred, prob = 0.975)) \n\n# extract the random effects for exemplars\nrandom_intc_matrix <- ranef(rand.slopes)$subject_id[, , \"Intercept\"] %>% \n  round(digits = 2) \n\n# extract the random effects for subject_id\n# slopes\nrandom_slope_matrix <- ranef(rand.slopes)$subject_id[, , \"log_RT_s\"] %>% \n  round(digits = 2)\n\n# intercepts\nrandom_intc_df <- data.frame(subject_id = row.names(random_intc_matrix), random_intc_matrix) %>% \n  select(subject_id, Estimate) %>% \n  rename(rintercept = Estimate)\n\n# wrangle into one df \nrandom_slope_df <- data.frame(subject_id = row.names(random_slope_matrix), random_slope_matrix) %>% \n  select(subject_id, Estimate) %>% \n  rename(rslope = Estimate) %>% \n  full_join(random_intc_df) %>% \n  # add population parameters and group-specific parameters\n  mutate(Intercept = round(fixef(rand.slopes)[1],2),\n         Slope = round(fixef(rand.slopes)[2],2),\n         adjusted_int = rintercept + Intercept,\n         adjusted_slope = rslope + Slope) %>% \n  mutate(log_RT = list(seq(-4, 6, 0.5))) %>% \n  unnest(log_RT) %>%\n  mutate(pred_m = plogis(adjusted_int + adjusted_slope*log_RT))\n\n# plot the individual regression lines on top of the population estimate\nggplot(data = predicted_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_agg,\n             position = position_jitter(height = 0.02), alpha = 0.2,\n             aes(x = log_RT_s, y = straight_numeric)) +\n  geom_line(data = random_slope_df, \n            aes(x = log_RT, y = pred_m, group = subject_id),\n            size = 0.5, alpha = 0.1) +\n  geom_line(size = 1.5, color = \"red\") +\n  xlim(-3,6) +\n  ylab(\"probability of straight trajectory\") +\n  labs(title = \"Model with varying slopes\",\n       subtitle = \"Thin lines are model estimates for each subject\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nYou can see that the model estimates for individual subjects are much less \"well behaved\" because we allow subjects to differ with regard to the effect of `log_RT_s`. As you can see, most lines slope downward just like the population estimate. They do so to different degrees, though. You can also see that some subjects show the opposite patters with upward sloping lines.\n\nThis variability lead to much more uncertainty regarding our population estimate. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = predicted_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_agg,\n             position = position_jitter(height = 0.02), alpha = 0.2,\n             aes(x = log_RT_s, y = straight_numeric)) +\n  geom_ribbon(aes(ymin = pred_low, ymax = pred_high), alpha=0.2) +\n  geom_line(size = 1.5, color = \"red\") +\n  xlim(-3,6) +\n  ylab(\"probability of straight trajectory\") +\n  labs(title = \"Model with varying slopes\",\n       subtitle = \"Ribbon represents the 95% CrI\")\n```\n\n::: {.cell-output-display}\n![](05a-hierarchical-models_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nOur take-home message is that we need to be aware of possible sources of random variation within our data. If we don't account for these groupings, we might draw overconfident or false conclusions.\n\nNow you might think back to most of the models we fit in past hands-on activities. And you are right. Most of these models ignored these sources of random variation. We made simplifications in order to learn about the tools themselves, but the results of these models are most likely overconfident estimates.\n\nSo how do we chose what random effect to include in the model and which ones not to choose. This is not a trivial question and it is still debated in the literature. To avoid overconfident claims, some researchers suggest to \"keep it maximal\" (See also https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/), i.e., to add all sources of random variation to the model that makes sense. That means, we add varying intercepts for all grouping variables we want to infer over, as well as varying slopes for predictors; at least where this is in principle reasonable.\n\nWhat does \"making sense\" or \"being in principle reasonable\" mean?\nOne way of thinking about this is whether a given random variation might make any sense conceptually speaking (e.g., is it imaginable that some people generally react faster than others? - totally!; is it imaginable that some participants are faster in certain tasks but slower in others? - yes!).\nIf you think about it in this way, you probably do want to include almost anything.\nAfter all, being \"imaginable\" is a rather weak requirement.\nAnd including all \"in principle imaginable\" sources of variation is prudent, and that's what motivates the idea to \"keep it maximal\".\n\nBut running very complex models with a lot of predictors and their corresponding random effect is computationally very expensive. \nSo keeping it maximal, might mean not being able to run it on *your* machine, or it might mean not being able to run it on *any* machine. \nThis is because in order to meaningfully estimate group-level variance, there needs to be a sufficient amount of data available in the first place. \nIf the model cannot estimate the variance, including certain random effect might be useless for inference, and worse: it may impede proper fitting of the model. \nSo one important step in your modelling workflow is finding the right model specification. \n\nOn particularly important case of data-sparsity should always be kept in mind: **you can only meaningfully estimate random effects licensed by the data set** (or sometimes this is phrased as \"licensed by the (experimental) design\"). For example, if your data is based on a between-subject experiment, i.e. each subject only contributes data to one level of a predictor, than it doesn't make sense to include by-subject random slopes for that predictor.\nYou cannot ever, by the design of the experiment, have data that would allow your to estimate this random variation.\n\nWe have all these models now which only differ in their random effect structure. Next week, when we look at model comparison, we will also directly compare these models (or model variations), so stay tuned.\n\nSo here we are. Being ready to perform Bayesian multilevel modeling with brms.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}