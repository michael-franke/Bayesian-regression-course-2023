---
title: "Bayesian regression: theory & practice"
subtitle: "02: Simple linear regression & categorical predictors"
author: "Michael Franke & Timo Roettger"
format: html
editor: visual
execute:
  error: false
  warning: false
  message: false
callout-appearance: simple
---

{{< include 00-preamble.qmd >}}

```{r}
dolphin <- aida::data_MT
```

# Exercises: categorical predictors

We want to regress `log RT` against the full combination of categorical factors `group`, `condition`, and `prototype_label`.

`log RT ~ group * condition * prototype_label`

The research hypotheses we would like to investigate are:

1.  Typical trials are faster than atypical ones.
2.  CoM trials are slower than the other kinds of trials (straight and curved) together, and respectively.
3.  'straight' trials are faster than 'curved' trials.
4.  Click trials are slower than touch trials.

But for this to work (without at least mildly informative priors), we would need to have a sufficient amount of observations in each cell. So, let's check:

```{r}
dolphin |> 
  mutate(group = as_factor(group),
         condition = as_factor(condition),
         prototype_label = as_factor(prototype_label)) |> 
  count(group, condition, prototype_label, .drop = FALSE) |> 
  arrange(n)
```

So, there are cells for which we have no observations at all. For simplicity, we therefore just lump all "change of mind"-type trajectories into one category:

```{r}
dolphin_prepped <- 
  dolphin |> 
  mutate(
    prototype_label = case_when(
     prototype_label %in% c('curved', 'straight') ~ prototype_label,
     TRUE ~ 'CoM'
    ),
    prototype_label = factor(prototype_label,
                             levels = c('straight', 'curved', 'CoM')))

dolphin_prepped |> 
  select(RT, prototype_label)
```

Here is a plot of the data to be analyzed:

```{r}
dolphin_prepped |> 
  ggplot(aes(x = log(RT), fill = condition)) +
  geom_density(alpha = 0.4) +
  facet_grid(group ~ prototype_label)
```

::: callout-caution
**Exercise 1a**

Use `brm()` to run a linear regression model for the data set `dolphin_prepped` and the formula:

`log RT ~ group * condition * prototype_label`

Set the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

fit <- brm(
  formula = log(RT) ~ group * condition * prototype_label,
  prior   = prior(student_t(1, 0, 3), class = "b"),
  data    = dolphin_prepped
  )
```

::: callout-caution
**Exercise 1b**

Plot the posteriors for population-level slope coefficients using the `tidybayes` package in order to:

1.  determine which combination of factor levels is the default cell
2.  check which coefficients have 95% CIs that do *not* include zero
3.  try to use this latter information to address any of our research hypotheses (stated above)
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"
tidybayes::summarise_draws(fit)
tidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |> 
  filter(.variable != "b_Intercept") |> 
  ggplot(aes(y = .variable, x = .value)) +
  tidybayes::stat_halfeye() +
  labs(x = "", y = "") +
  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,
               lty = "dashed")

# the default cell is for click-atypical-straight

# coeffiencents with 95% CIs that do not include zero are:
#   grouptouch, conditionTypical, prototype_labelCoM

# none of these give us direct information about our research hypotheses
```

::: callout-caution
**Exercise 1c**

Use the `faintr` package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# 1. Typical trials are faster than atypical ones.
# -> There is overwhelming evidence that this is true 
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = condition == 'Typical',
  higher = condition == 'Atypical'
)

# 2. CoM trials are slower than the other kinds of trials 
#    (straight and curved) together, and respectively.
# -> There is overwhelming evidence that this is true 
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = prototype_label != 'CoM',
  higher = prototype_label == 'CoM'
)

faintr::compare_groups(
  fit,
  lower  = prototype_label == 'straight',
  higher = prototype_label == 'CoM'
)

# 3. 'straight' trials are faster than 'curved' trials.
# -> There is no evidence for this hypothesis
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = prototype_label == 'straight',
  higher = prototype_label == 'curved'
)

# 4. Click trials are slower than touch trials.
# -> There is overwhelming evidence that this is true 
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = group == 'touch',
  higher = group == 'click'
)
```

## Exercises: metric and categorical predictors

Here is an aggregated data set `dolphin_agg2` for you.

```{r exercise2}

# aggregate
dolphin_agg2 <- dolphin %>% 
  filter(correct == 1) %>% 
  group_by(exemplar, group, condition) %>% 
  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),
                   RT = median(RT, na.rm = TRUE)) %>% 
  mutate(log_RT = log(RT))

```

::: callout-caution
**Exercise 2a**

Run a model predicting MAD based on *standardized* `log_RT`, `group`, `condition`, and *their three-way interaction*. Set a seed = 999.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# standardize
dolphin_agg2$log_RT_s <- scale(dolphin_agg2$log_RT, scale = TRUE)

# model
model3 = brm(
  MAD ~ log_RT_s * group * condition, 
  data = dolphin_agg2,
  iter = 2000,
  chains = 4,
  seed = 999
  )

```

::: callout-caution
**Exercise 2b**

Look at the output. Extract posterior means and 95% CrIs for the following predictor level combinations. One row corresponds to one concrete combination of levels. (Tip: check your results by plotting them against the data)

-   Combination1: log_RT_s == 0; group == click; condition == Atypical
-   Combination2: log_RT_s == 0; group == touch; condition == Atypical
-   Combination3: log_RT_s == 1; group == touch; condition == Typical
-   Combination4: log_RT_s == 2; group == touch; condition == Atypical
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

posteriors3a <- model3 %>%
  spread_draws(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`,
               `b_log_RT_s:grouptouch:conditionTypical`
               ) %>% 
  mutate(Combination1 = b_Intercept + (0 * b_log_RT_s),
         Combination2 = b_Intercept + (0 * b_log_RT_s) + b_grouptouch,
         Combination3 = b_Intercept + (1 * b_log_RT_s) + b_grouptouch + 
           b_conditionTypical + `b_grouptouch:conditionTypical` + (1 * `b_log_RT_s:grouptouch`) + 
           (1 * `b_log_RT_s:conditionTypical`) + (1 * `b_log_RT_s:grouptouch:conditionTypical`),
         Combination4 = b_Intercept + (2 * b_log_RT_s) + b_grouptouch + 
           (2 * `b_log_RT_s:grouptouch`)) %>% 
  dplyr::select(Combination1, Combination2,
                Combination3, Combination4) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  group_by(parameter) %>% 
  summarise(mean_posterior = mean(posterior),
            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],
            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])

posteriors3a

```

::: callout-caution
**Exercise 2c**

Define the following priors and run the model3 again:

-   log_RT_s: student-t (df = 3, mean = 0, sd = 30)
-   grouptouch: student-t (df = 3, mean = 100, sd = 200)
-   conditionTypical: student-t (df = 3, mean = 0, sd = 200)
-   log_RT_s:grouptouch: normal (mean = 0, sd = 30)
-   log_RT_s:conditionTypical: normal (mean = 0, sd = 30)
-   grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 200)
-   log_RT_s:grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 30)
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

priors_model3 <- c(
   set_prior("student_t(3,0,30)", class = "b", coef = "log_RT_s"),
   set_prior("student_t(3,100,200)", class = "b", coef = "grouptouch"),
   set_prior("student_t(3,0,200)", class = "b", coef = "conditionTypical"),
   set_prior("normal(0,30)", class = "b", coef = "log_RT_s:grouptouch"),
   set_prior("normal(0,30)", class = "b", coef = "log_RT_s:conditionTypical"),
   set_prior("student_t(3,0,200)", class = "b", coef = "grouptouch:conditionTypical"),
   set_prior("student_t(3,0,30)", class = "b", coef = "log_RT_s:grouptouch:conditionTypical")
)

# model
model3b = brm(
  MAD ~ log_RT_s * group * condition, 
  data = dolphin_agg2,
  iter = 2000,
  chains = 4,
  prior = priors_model3
  )

```

::: callout-caution
**Exercise 2d**

Compare the two posterior estimates from model3 and model3b. What has changed?
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# extract posteriors for model2
posteriors3a <- model3 %>%
  spread_draws(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  select(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  group_by(parameter)

# extract posteriors for model2b
posteriors3b <- model3b %>%
  spread_draws(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  select(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  group_by(parameter)

# plot posteriors for model2
ggplot(posteriors3a, aes(x = posterior, y = parameter)) + 
    # plot density 
    geom_halfeyeh(.width = 0.95) +
    # add axes titles
    xlab("\nMAD") +
    ylab("") +
    # add line for the value zero
    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,
                 lty = "dashed") +
    scale_x_continuous(limits = c(-250, 250))
  
# plot posteriors for model2b
ggplot(posteriors3b, aes(x = posterior, y = parameter)) + 
    # plot density 
    geom_halfeyeh(.width = 0.95) +
    # add axes titles
    xlab("\nMAD") +
    ylab("") +
    # add line for the value zero
    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,
                 lty = "dashed") +
    scale_x_continuous(limits = c(-250, 250))

# ANSWER:The model output does not change much. Overall, the posteriors are a little tighter and closer to zero for model3b

```

::: callout-caution
**Exercise 2a**

Suppose you have the following aggregated data set and want to run the following linear model: `AUC ~ condition`

```{r}

# aggregate
dolphin_agg3 <- dolphin %>% 
  filter(correct == 1) %>% 
  group_by(subject_id, condition) %>%
  dplyr::summarize(AUC = median(AUC, na.rm = TRUE)) 

dolphin_agg3$AUC_s <- scale(dolphin_agg3$AUC, scale = TRUE)

```

Deviation code the effect of condition, such that the Intercept gives you the grand average of `AUC_s` and the coefficient for condition gives you the difference between Atypical + Typical exemplars. Check last week's reading of Bodo Winter's book again (or google).

Specify an agnostic prior for the effect of condition and run the model from above (set `seed = 333`).
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# contrast code condition
c <- contr.treatment(2)

# divide by 2 for it to represent the difference
my.coding <- matrix(rep(1/2, 2), ncol = 1)
my.simple <- c - my.coding

# make factor
dolphin_agg3$condition <- as.factor(dolphin_agg3$condition)
# associate with contrast
contrasts(dolphin_agg3$condition) = my.simple

priors_agnostic <- c(
   # Atypical < Typical
   set_prior("normal(0, 3)", class = "b", coef = "condition2")
)

model4 = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_agnostic
  )

```

::: callout-caution
**Exercise 2f**

Now suppose you have three people who want to encode their subjective beliefs about whether and how group and condition affect `AUC_s`. To keep your solutions comparable, we assume prior beliefs are normally distributed and there are three types of beliefs:

1.  A strong belief in a directional relationship: The person assumes that there is a difference between two conditions (A\>B). The mean of the assumed differences is 3 units of AUC_s with a SD of 0.5.

2.  An agnostic belief in a directional relationship: Both A\>B and B\>A are plausible, but uncertainty is high. The mean of the most plausible distribution is 0 with a SD of 3, i.e. a rather wide distribution, allowing effects in both directions.

Here are three researchers and their prior beliefs:

*Michael* holds strong prior beliefs that Typical exemplars exhibit less curvature than Atypical exemplars.

*Nina* is agnostic about the effect of condition on `AUC_s`.

As opposed to Michael, *Jones* holds strong prior beliefs that Typical exemplars exhibit MORE curvature than Atypical exemplars.

Specify priors for Michael, Nina, and Jones, and run models (set seed = 323) for all of these scenarios. Look at the results (maybe plot the posteriors if that helps you) and briefly describe how the priors affected the posteriors.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

priors_Michael <- c(
   # Atypical > Typical
   set_prior("normal(-3, 0.5)", class = "b", coef = "condition2")
)

priors_Nina <- c(
   # Atypical < Typical
   set_prior("normal(0, 3)", class = "b", coef = "condition2")
)

priors_Jones <- c(
      set_prior("normal(3, 0.5)", class = "b", coef = "condition2")
)


# model
model5_Michael = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_Michael
  )

model5_Nina = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_Nina
  )

model5_Jones = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_Jones
  )

# extract posteriors

## Michael
posteriors_Michael <- model5_Michael %>%
  spread_draws(b_condition2) %>% 
  select(b_condition2) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  mutate(model = "Michael")
  

## Nina
posteriors_Nina <- model5_Nina %>%
  spread_draws(b_condition2) %>% 
  select(b_condition2) %>% 
  gather(key = "parameter", value = "posterior")  %>% 
  mutate(model = "Nina")

## Jones
posteriors_Jones <- model5_Jones %>%
  spread_draws(b_condition2) %>% 
  select(b_condition2) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  mutate(model = "Jones")

# add to one df
posteriors_all <- rbind(posteriors_Michael, posteriors_Nina, posteriors_Jones)

# plot posteriors for all models
ggplot(posteriors_all, aes(x = posterior, y = parameter, fill = model, color = model)) + 
    # plot density 
    geom_halfeyeh(.width = 0.95, alpha = 0.4) +
    facet_grid(model~ .) +
    # add axes titles
    xlab("\nAUC") +
    ylab("") +
    # add line for the value zero
    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf, color = "black",
                 lty = "dashed") +
    scale_x_continuous(limits = c(-1.5, 0.5))

#ANSWER: The agnostic Nina serves as a baseline. With a weakly informative prior, the likelihood dominates the posterior.
# Michael has a strong preconception that Atypical exemplars elicit larger AUC values and in comparison to Jones, the posterior distribution is slightly shifted more toward negative values.
# Jones has a strong preconception that Typical exemplars elicit larger AUC values and in comparison to Jones and Michael, the posterior distribution is slightly shifted AWAY from negative values.
# Here, the priors, although encoding strong prior beliefs, have only little impact on the posteriors but do shift posteriors toward the priors.

```
