---
title: "Bayesian regression: theory & practice"
subtitle: "02: Simple linear regression & categorical predictors"
author: "Michael Franke"
format: html
editor: visual
execute:
  error: false
  warning: false
  message: false
callout-appearance: simple
---

{{< include 00-preamble.qmd >}}

```{r}
dolphin <- aida::data_MT
```

# Prior and posterior predictives

Here is an example for how we can obtain draws from the posterior predictive distribution using `tidybayes::predicted_draws`. We are using the fit of a linear model to the average world temperature for the year 2025 to 2024.

```{r}

plot_predictPriPost <- function(prior_spec, ndraws = 1000) {
  
  # get the posterior fit
  fit <- brm(
    avg_temp ~ year,
    prior = prior_spec,
    data = aida::data_WorldTemp,
    silent = TRUE,
    refresh = 0
  )
  
  # retrieve prior samples from the posterior fit
  fit_prior_only <- update(
    fit,
    silent = TRUE,
    refresh = 0,
    sample_prior = "only"
  )
  
  get_predictions <- function(fit_object, type = "prior prediction") {
    
    tidybayes::add_linpred_draws(
      fit_object, 
      newdata = tibble(year = aida::data_WorldTemp$year),
      ndraws = ndraws,
      value = 'avg_tmp'
    ) |> 
      ungroup() |> 
      select(year, .draw, avg_tmp) |> 
      mutate(type = type)
    
  }
  
  get_predictions(fit, "posterior prediction") |> 
    rbind(get_predictions(fit_prior_only, "prior prediction")) |> 
    mutate(type = factor(type, levels = c("prior prediction", "posterior prediction"))) |> 
    ggplot() + 
    facet_grid(type ~ ., scales = "free") +
    geom_line(aes(x = year, y = avg_tmp, group = .draw), 
              color = "gray", alpha = 0.3) +
    geom_point(data = aida::data_WorldTemp, 
               aes(x = year, y = avg_temp), color = project_colors[2], size = 1, alpha = 0.8) +
    ylab("average temperature")
}

prior_baseline <- c(prior("normal(0, 0.02)", class = "b"),
                    prior("student_t(3, 8, 5)", class = "Intercept"))

prior_opinionated <- c(prior("normal(0.2, 0.05)", class = "b"),
                       prior("student_t(3, 8, 5)", class = "Intercept"))

prior_crazy <- c(prior("normal(-1, 0.005)", class = "b"),
                 prior("student_t(3, 8, 5)", class = "Intercept"))

plot_predictPriPost(prior_baseline)
# ggsave("pics/02a-predictives-baseline.pdf", width = 5, height = 8)

plot_predictPriPost(prior_opinionated)
# ggsave("pics/02b-predictives-opinionated.pdf", width = 5, height = 8)

plot_predictPriPost(prior_crazy)
# ggsave("pics/02c-predictives-crazy.pdf", width = 5, height = 8)

```

::: callout-caution
**Exercise 0**

Play around with different prior specifications, and inspect the resulting prior and posterior predictions.
:::

# Exercises: linear regression w/ metric & categorical predictor

## Exercise 1:

Here is an aggregated data set `dolphin_agg` for you.

```{r exercise1}

# aggregate
dolphin_agg <- dolphin %>% 
  group_by(group, exemplar) %>% 
  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),
                   RT = median(RT, na.rm = TRUE)) %>% 
  mutate(log_RT = log(RT))

```

::: callout-caution
**Exercise 1a**

Standardize ("z-transform") `log_RT` such that the mean is at zero and 1 unit corresponds to the standard deviation. Name it `log_RT_s`.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

dolphin_agg$log_RT_s <- scale(dolphin_agg$log_RT, scale = TRUE)

```

::: callout-caution
**Exercise 1b**

Run a linear model with `brms` that predicts `MAD` based on `log_RT_s`, `group`, and their two-way interaction. Run 2000 iterations and 6 chains.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

model1 = brm(
  MAD ~ log_RT_s * group, 
  data = dolphin_agg,
  iter = 2000,
  chains = 6
  )

```

::: callout-caution
**Exercise 1c**

Plot `MAD` (y) against `log_RT_s` (x) in a scatter plot and color-code for `group`. Plot the regression lines for the click and the touch group into the plot and don't forget to take possible interactions into account.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# extract posterior means for model coefficients
Intercept = summary(model1)$fixed[1,1]
log_RT = summary(model1)$fixed[2,1]
group = summary(model1)$fixed[3,1]
interaction = summary(model1)$fixed[4,1]

# plot
ggplot(data = dolphin_agg, 
       aes(x = log_RT_s, 
           y = MAD, 
           color = group)) + 
  geom_point(size = 3, alpha = 0.3) +
  geom_vline(xintercept = 0, lty = "dashed") +
  geom_abline(intercept = Intercept, slope = log_RT, color = "#E69F00", size = 2) +
  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, color = "#56B4E9", size = 2) 

```

::: callout-caution
**Exercise 1d**

Specify very skeptic priors for all three coefficients. Use a normal distribution with mean = 0, and sd = 10. Rerun the model with those priors.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# specify priors
priors_model2 <- c(
  set_prior("normal(0,10)", class = "b", coef = "log_RT_s"),
  set_prior("normal(0,10)", class = "b", coef = "grouptouch"),
  set_prior("normal(0,10)", class = "b", coef = "log_RT_s:grouptouch")
)

# model
model2 = brm(
  MAD ~ log_RT_s * group, 
  data = dolphin_agg,
  iter = 2000,
  chains = 6,
  prior = priors_model2
  )

```

::: callout-caution
**Exercise 1e**

Compare the model output of model1 to model2. What are the differences and what are the reasons for these differences?
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# We can compare the model predictions by looking at the coefficients / plotting them:
summary(model1)
summary(model2)

# extract posterior means for model coefficients
Intercept = summary(model2)$fixed[1,1]
log_RT = summary(model2)$fixed[2,1]
group = summary(model2)$fixed[3,1]
interaction = summary(model2)$fixed[4,1]

# plot
ggplot(data = dolphin_agg, 
       aes(x = log_RT_s, 
           y = MAD, 
           color = group)) + 
  geom_point(size = 3, alpha = 0.3) +
  geom_vline(xintercept = 0, lty = "dashed") +
  geom_abline(intercept = Intercept, slope = log_RT, color = "#E69F00", size = 2) +
  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, color = "#56B4E9", size = 2) 

# ANSWER: the magnitude of the coefficients is much smaller in model2, with the interaction term being close to zero. As a result, the lines in the plot are closer together and run in parallel. The reason for this change lies in the priors. We defined the priors of model2 rather narrowly, down weighing  data points larger or smaller than zero. This is a case of the prior dominating the posterior.

```

## Exercise 2

Here is an aggregated data set `dolphin_agg2` for you.

```{r exercise2}

# aggregate
dolphin_agg2 <- dolphin %>% 
  filter(correct == 1) %>% 
  group_by(exemplar, group, condition) %>% 
  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),
                   RT = median(RT, na.rm = TRUE)) %>% 
  mutate(log_RT = log(RT))

```

::: callout-caution
**Exercise 2a**

Run a model predicting MAD based on *standardized* `log_RT`, `group`, `condition`, and *their three-way interaction*. Set a seed = 999.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# standardize
dolphin_agg2$log_RT_s <- scale(dolphin_agg2$log_RT, scale = TRUE)

# model
model3 = brm(
  MAD ~ log_RT_s * group * condition, 
  data = dolphin_agg2,
  iter = 2000,
  chains = 4,
  seed = 999
  )

```

::: callout-caution
**Exercise 2b**

Look at the output. Extract posterior means and 95% CrIs for the following predictor level combinations. One row corresponds to one concrete combination of levels. (Tip: check your results by plotting them against the data)

-   Combination1: log_RT_s == 0; group == click; condition == Atypical
-   Combination2: log_RT_s == 0; group == touch; condition == Atypical
-   Combination3: log_RT_s == 1; group == touch; condition == Typical
-   Combination4: log_RT_s == 2; group == touch; condition == Atypical
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

posteriors3a <- model3 %>%
  spread_draws(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`,
               `b_log_RT_s:grouptouch:conditionTypical`
               ) %>% 
  mutate(Combination1 = b_Intercept + (0 * b_log_RT_s),
         Combination2 = b_Intercept + (0 * b_log_RT_s) + b_grouptouch,
         Combination3 = b_Intercept + (1 * b_log_RT_s) + b_grouptouch + 
           b_conditionTypical + `b_grouptouch:conditionTypical` + (1 * `b_log_RT_s:grouptouch`) + 
           (1 * `b_log_RT_s:conditionTypical`) + (1 * `b_log_RT_s:grouptouch:conditionTypical`),
         Combination4 = b_Intercept + (2 * b_log_RT_s) + b_grouptouch + 
           (2 * `b_log_RT_s:grouptouch`)) %>% 
  dplyr::select(Combination1, Combination2,
                Combination3, Combination4) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  group_by(parameter) %>% 
  summarise(mean_posterior = mean(posterior),
            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],
            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])

posteriors3a

```

::: callout-caution
**Exercise 2c**

Define the following priors and run the model3 again:

-   log_RT_s: student-t (df = 3, mean = 0, sd = 30)
-   grouptouch: student-t (df = 3, mean = 100, sd = 200)
-   conditionTypical: student-t (df = 3, mean = 0, sd = 200)
-   log_RT_s:grouptouch: normal (mean = 0, sd = 30)
-   log_RT_s:conditionTypical: normal (mean = 0, sd = 30)
-   grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 200)
-   log_RT_s:grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 30)
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

priors_model3 <- c(
   set_prior("student_t(3,0,30)", class = "b", coef = "log_RT_s"),
   set_prior("student_t(3,100,200)", class = "b", coef = "grouptouch"),
   set_prior("student_t(3,0,200)", class = "b", coef = "conditionTypical"),
   set_prior("normal(0,30)", class = "b", coef = "log_RT_s:grouptouch"),
   set_prior("normal(0,30)", class = "b", coef = "log_RT_s:conditionTypical"),
   set_prior("student_t(3,0,200)", class = "b", coef = "grouptouch:conditionTypical"),
   set_prior("student_t(3,0,30)", class = "b", coef = "log_RT_s:grouptouch:conditionTypical")
)

# model
model3b = brm(
  MAD ~ log_RT_s * group * condition, 
  data = dolphin_agg2,
  iter = 2000,
  chains = 4,
  prior = priors_model3
  )

```

::: callout-caution
**Exercise 2d**

Compare the two posterior estimates from model3 and model3b. What has changed?
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# extract posteriors for model2
posteriors3a <- model3 %>%
  spread_draws(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  select(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  group_by(parameter)

# extract posteriors for model2b
posteriors3b <- model3b %>%
  spread_draws(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  select(b_Intercept, b_log_RT_s,
               b_grouptouch, b_conditionTypical,
               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,
               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  group_by(parameter)

# plot posteriors for model2
ggplot(posteriors3a, aes(x = posterior, y = parameter)) + 
    # plot density 
    geom_halfeyeh(.width = 0.95) +
    # add axes titles
    xlab("\nMAD") +
    ylab("") +
    # add line for the value zero
    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,
                 lty = "dashed") +
    scale_x_continuous(limits = c(-250, 250))
  
# plot posteriors for model2b
ggplot(posteriors3b, aes(x = posterior, y = parameter)) + 
    # plot density 
    geom_halfeyeh(.width = 0.95) +
    # add axes titles
    xlab("\nMAD") +
    ylab("") +
    # add line for the value zero
    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,
                 lty = "dashed") +
    scale_x_continuous(limits = c(-250, 250))

# ANSWER:The model output does not change much. Overall, the posteriors are a little tighter and closer to zero for model3b

```

::: callout-caution
**Exercise 2a**

Suppose you have the following aggregated data set and want to run the following linear model: `AUC ~ condition`

```{r}

# aggregate
dolphin_agg3 <- dolphin %>% 
  filter(correct == 1) %>% 
  group_by(subject_id, condition) %>%
  dplyr::summarize(AUC = median(AUC, na.rm = TRUE)) 

dolphin_agg3$AUC_s <- scale(dolphin_agg3$AUC, scale = TRUE)

```

Deviation code the effect of condition, such that the Intercept gives you the grand average of `AUC_s` and the coefficient for condition gives you the difference between Atypical + Typical exemplars. Check last week's reading of Bodo Winter's book again (or google).

Specify an agnostic prior for the effect of condition and run the model from above (set `seed = 333`).
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# contrast code condition
c <- contr.treatment(2)

# divide by 2 for it to represent the difference
my.coding <- matrix(rep(1/2, 2), ncol = 1)
my.simple <- c - my.coding

# make factor
dolphin_agg3$condition <- as.factor(dolphin_agg3$condition)
# associate with contrast
contrasts(dolphin_agg3$condition) = my.simple

priors_agnostic <- c(
   # Atypical < Typical
   set_prior("normal(0, 3)", class = "b", coef = "condition2")
)

model4 = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_agnostic
  )

```

::: callout-caution
**Exercise 2f**

Now suppose you have three people who want to encode their subjective beliefs about whether and how group and condition affect `AUC_s`. To keep your solutions comparable, we assume prior beliefs are normally distributed and there are three types of beliefs:

1.  A strong belief in a directional relationship: The person assumes that there is a difference between two conditions (A\>B). The mean of the assumed differences is 3 units of AUC_s with a SD of 0.5.

2.  An agnostic belief in a directional relationship: Both A\>B and B\>A are plausible, but uncertainty is high. The mean of the most plausible distribution is 0 with a SD of 3, i.e. a rather wide distribution, allowing effects in both directions.

Here are three researchers and their prior beliefs:

*Michael* holds strong prior beliefs that Typical exemplars exhibit less curvature than Atypical exemplars.

*Nina* is agnostic about the effect of condition on `AUC_s`.

As opposed to Michael, *Jones* holds strong prior beliefs that Typical exemplars exhibit MORE curvature than Atypical exemplars.

Specify priors for Michael, Nina, and Jones, and run models (set seed = 323) for all of these scenarios. Look at the results (maybe plot the posteriors if that helps you) and briefly describe how the priors affected the posteriors.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

priors_Michael <- c(
   # Atypical > Typical
   set_prior("normal(-3, 0.5)", class = "b", coef = "condition2")
)

priors_Nina <- c(
   # Atypical < Typical
   set_prior("normal(0, 3)", class = "b", coef = "condition2")
)

priors_Jones <- c(
      set_prior("normal(3, 0.5)", class = "b", coef = "condition2")
)


# model
model5_Michael = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_Michael
  )

model5_Nina = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_Nina
  )

model5_Jones = brm(
  AUC_s ~ condition, 
  data = dolphin_agg3,
  iter = 2000,
  chains = 4,
  seed = 333,
  prior = priors_Jones
  )

# extract posteriors

## Michael
posteriors_Michael <- model5_Michael %>%
  spread_draws(b_condition2) %>% 
  select(b_condition2) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  mutate(model = "Michael")
  

## Nina
posteriors_Nina <- model5_Nina %>%
  spread_draws(b_condition2) %>% 
  select(b_condition2) %>% 
  gather(key = "parameter", value = "posterior")  %>% 
  mutate(model = "Nina")

## Jones
posteriors_Jones <- model5_Jones %>%
  spread_draws(b_condition2) %>% 
  select(b_condition2) %>% 
  gather(key = "parameter", value = "posterior") %>% 
  mutate(model = "Jones")

# add to one df
posteriors_all <- rbind(posteriors_Michael, posteriors_Nina, posteriors_Jones)

# plot posteriors for all models
ggplot(posteriors_all, aes(x = posterior, y = parameter, fill = model, color = model)) + 
    # plot density 
    geom_halfeyeh(.width = 0.95, alpha = 0.4) +
    facet_grid(model~ .) +
    # add axes titles
    xlab("\nAUC") +
    ylab("") +
    # add line for the value zero
    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf, color = "black",
                 lty = "dashed") +
    scale_x_continuous(limits = c(-1.5, 0.5))

#ANSWER: The agnostic Nina serves as a baseline. With a weakly informative prior, the likelihood dominates the posterior.
# Michael has a strong preconception that Atypical exemplars elicit larger AUC values and in comparison to Jones, the posterior distribution is slightly shifted more toward negative values.
# Jones has a strong preconception that Typical exemplars elicit larger AUC values and in comparison to Jones and Michael, the posterior distribution is slightly shifted AWAY from negative values.
# Here, the priors, although encoding strong prior beliefs, have only little impact on the posteriors but do shift posteriors toward the priors.

```

# Exercises: categorical predictors

We want to regress `log RT` against the full combination of categorical factors `group`, `condition`, and `prototype_label`.

`log RT ~ group * condition * prototype_label`

The research hypotheses we would like to investigate are:

1.  Typical trials are faster than atypical ones.
2.  CoM trials are slower than the other kinds of trials (straight and curved) together, and respectively.
3.  'straight' trials are faster than 'curved' trials.
4.  Click trials are slower than touch trials.

But for this to work (without at least mildly informative priors), we would need to have a sufficient amount of observations in each cell. So, let's check:

```{r}
dolphin |> 
  mutate(group = as_factor(group),
         condition = as_factor(condition),
         prototype_label = as_factor(prototype_label)) |> 
  count(group, condition, prototype_label, .drop = FALSE) |> 
  arrange(n)
```

So, there are cells for which we have no observations at all. For simplicity, we therefore just lump all "change of mind"-type trajectories into one category:

```{r}
dolphin_prepped <- 
  dolphin |> 
  mutate(
    prototype_label = case_when(
     prototype_label %in% c('curved', 'straight') ~ prototype_label,
     TRUE ~ 'CoM'
    ),
    prototype_label = factor(prototype_label,
                             levels = c('straight', 'curved', 'CoM')))

dolphin_prepped |> 
  select(RT, prototype_label)
```

Here is a plot of the data to be analyzed:

```{r}
dolphin_prepped |> 
  ggplot(aes(x = log(RT), fill = condition)) +
  geom_density(alpha = 0.4) +
  facet_grid(group ~ prototype_label)
```

::: callout-caution
**Exercise 3a**

Use `brm()` to run a linear regression model for the data set `dolphin_prepped` and the formula:

`log RT ~ group * condition * prototype_label`

Set the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

fit <- brm(
  formula = log(RT) ~ group * condition * prototype_label,
  prior   = prior(student_t(1, 0, 3), class = "b"),
  data    = dolphin_prepped
  )
```

::: callout-caution
**Exercise 3b**

Plot the posteriors for population-level slope coefficients using the `tidybayes` package in order to:

1.  determine which combination of factor levels is the default cell
2.  check which coefficients have 95% CIs that do *not* include zero
3.  try to use this latter information to address any of our research hypotheses (stated above)
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"
tidybayes::summarise_draws(fit)
tidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |> 
  filter(.variable != "b_Intercept") |> 
  ggplot(aes(y = .variable, x = .value)) +
  tidybayes::stat_halfeye() +
  labs(x = "", y = "") +
  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,
               lty = "dashed")

# the default cell is for click-atypical-straight

# coeffiencents with 95% CIs that do not include zero are:
#   grouptouch, conditionTypical, prototype_labelCoM

# none of these give us direct information about our research hypotheses
```

::: callout-caution
**Exercise 3c**

Use the `faintr` package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.
:::

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show solution"

# 1. Typical trials are faster than atypical ones.
# -> There is overwhelming evidence that this is true 
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = condition == 'Typical',
  higher = condition == 'Atypical'
)

# 2. CoM trials are slower than the other kinds of trials 
#    (straight and curved) together, and respectively.
# -> There is overwhelming evidence that this is true 
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = prototype_label != 'CoM',
  higher = prototype_label == 'CoM'
)

faintr::compare_groups(
  fit,
  lower  = prototype_label == 'straight',
  higher = prototype_label == 'CoM'
)

# 3. 'straight' trials are faster than 'curved' trials.
# -> There is no evidence for this hypothesis
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = prototype_label == 'straight',
  higher = prototype_label == 'curved'
)

# 4. Click trials are slower than touch trials.
# -> There is overwhelming evidence that this is true 
#    (given the data and the model).

faintr::compare_groups(
  fit,
  lower  = group == 'touch',
  higher = group == 'click'
)
```
