[
  {
    "objectID": "practice-sheets/02d-linReg-multiple-catPredictors.html",
    "href": "practice-sheets/02d-linReg-multiple-catPredictors.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/02d-linReg-multiple-catPredictors.html#exercises-metric-and-categorical-predictors",
    "href": "practice-sheets/02d-linReg-multiple-catPredictors.html#exercises-metric-and-categorical-predictors",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises: metric and categorical predictors",
    "text": "Exercises: metric and categorical predictors\nHere is an aggregated data set dolphin_agg2 for you.\n\n# aggregate\ndolphin_agg2 <- dolphin %>% \n  filter(correct == 1) %>% \n  group_by(exemplar, group, condition) %>% \n  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),\n                   RT = median(RT, na.rm = TRUE)) %>% \n  mutate(log_RT = log(RT))\n\n\n\n\n\n\n\nExercise 2a\nRun a model predicting MAD based on standardized log_RT, group, condition, and their three-way interaction. Set a seed = 999.\n\n\n\n\n\nShow solution\n# standardize\ndolphin_agg2$log_RT_s <- scale(dolphin_agg2$log_RT, scale = TRUE)\n\n# model\nmodel3 = brm(\n  MAD ~ log_RT_s * group * condition, \n  data = dolphin_agg2,\n  iter = 2000,\n  chains = 4,\n  seed = 999\n  )\n\n\n\n\n\n\n\n\nExercise 2b\nLook at the output. Extract posterior means and 95% CrIs for the following predictor level combinations. One row corresponds to one concrete combination of levels. (Tip: check your results by plotting them against the data)\n\nCombination1: log_RT_s == 0; group == click; condition == Atypical\nCombination2: log_RT_s == 0; group == touch; condition == Atypical\nCombination3: log_RT_s == 1; group == touch; condition == Typical\nCombination4: log_RT_s == 2; group == touch; condition == Atypical\n\n\n\n\n\n\nShow solution\nposteriors3a <- model3 %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`,\n               `b_log_RT_s:grouptouch:conditionTypical`\n               ) %>% \n  mutate(Combination1 = b_Intercept + (0 * b_log_RT_s),\n         Combination2 = b_Intercept + (0 * b_log_RT_s) + b_grouptouch,\n         Combination3 = b_Intercept + (1 * b_log_RT_s) + b_grouptouch + \n           b_conditionTypical + `b_grouptouch:conditionTypical` + (1 * `b_log_RT_s:grouptouch`) + \n           (1 * `b_log_RT_s:conditionTypical`) + (1 * `b_log_RT_s:grouptouch:conditionTypical`),\n         Combination4 = b_Intercept + (2 * b_log_RT_s) + b_grouptouch + \n           (2 * `b_log_RT_s:grouptouch`)) %>% \n  dplyr::select(Combination1, Combination2,\n                Combination3, Combination4) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter) %>% \n  summarise(mean_posterior = mean(posterior),\n            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],\n            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])\n\nposteriors3a\n\n\n\n\n\n\n\n\nExercise 2c\nDefine the following priors and run the model3 again:\n\nlog_RT_s: student-t (df = 3, mean = 0, sd = 30)\ngrouptouch: student-t (df = 3, mean = 100, sd = 200)\nconditionTypical: student-t (df = 3, mean = 0, sd = 200)\nlog_RT_s:grouptouch: normal (mean = 0, sd = 30)\nlog_RT_s:conditionTypical: normal (mean = 0, sd = 30)\ngrouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 200)\nlog_RT_s:grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 30)\n\n\n\n\n\n\nShow solution\npriors_model3 <- c(\n   set_prior(\"student_t(3,0,30)\", class = \"b\", coef = \"log_RT_s\"),\n   set_prior(\"student_t(3,100,200)\", class = \"b\", coef = \"grouptouch\"),\n   set_prior(\"student_t(3,0,200)\", class = \"b\", coef = \"conditionTypical\"),\n   set_prior(\"normal(0,30)\", class = \"b\", coef = \"log_RT_s:grouptouch\"),\n   set_prior(\"normal(0,30)\", class = \"b\", coef = \"log_RT_s:conditionTypical\"),\n   set_prior(\"student_t(3,0,200)\", class = \"b\", coef = \"grouptouch:conditionTypical\"),\n   set_prior(\"student_t(3,0,30)\", class = \"b\", coef = \"log_RT_s:grouptouch:conditionTypical\")\n)\n\n# model\nmodel3b = brm(\n  MAD ~ log_RT_s * group * condition, \n  data = dolphin_agg2,\n  iter = 2000,\n  chains = 4,\n  prior = priors_model3\n  )\n\n\n\n\n\n\n\n\nExercise 2d\nCompare the two posterior estimates from model3 and model3b. What has changed?\n\n\n\n\n\nShow solution\n# extract posteriors for model2\nposteriors3a <- model3 %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  select(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter)\n\n# extract posteriors for model2b\nposteriors3b <- model3b %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  select(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter)\n\n# plot posteriors for model2\nggplot(posteriors3a, aes(x = posterior, y = parameter)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nMAD\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-250, 250))\n  \n# plot posteriors for model2b\nggplot(posteriors3b, aes(x = posterior, y = parameter)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nMAD\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-250, 250))\n\n# ANSWER:The model output does not change much. Overall, the posteriors are a little tighter and closer to zero for model3b\n\n\n\n\n\n\n\n\nExercise 2a\nSuppose you have the following aggregated data set and want to run the following linear model: AUC ~ condition\n\n# aggregate\ndolphin_agg3 <- dolphin %>% \n  filter(correct == 1) %>% \n  group_by(subject_id, condition) %>%\n  dplyr::summarize(AUC = median(AUC, na.rm = TRUE)) \n\ndolphin_agg3$AUC_s <- scale(dolphin_agg3$AUC, scale = TRUE)\n\nDeviation code the effect of condition, such that the Intercept gives you the grand average of AUC_s and the coefficient for condition gives you the difference between Atypical + Typical exemplars. Check last week’s reading of Bodo Winter’s book again (or google).\nSpecify an agnostic prior for the effect of condition and run the model from above (set seed = 333).\n\n\n\n\n\nShow solution\n# contrast code condition\nc <- contr.treatment(2)\n\n# divide by 2 for it to represent the difference\nmy.coding <- matrix(rep(1/2, 2), ncol = 1)\nmy.simple <- c - my.coding\n\n# make factor\ndolphin_agg3$condition <- as.factor(dolphin_agg3$condition)\n# associate with contrast\ncontrasts(dolphin_agg3$condition) = my.simple\n\npriors_agnostic <- c(\n   # Atypical < Typical\n   set_prior(\"normal(0, 3)\", class = \"b\", coef = \"condition2\")\n)\n\nmodel4 = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_agnostic\n  )\n\n\n\n\n\n\n\n\nExercise 2f\nNow suppose you have three people who want to encode their subjective beliefs about whether and how group and condition affect AUC_s. To keep your solutions comparable, we assume prior beliefs are normally distributed and there are three types of beliefs:\n\nA strong belief in a directional relationship: The person assumes that there is a difference between two conditions (A>B). The mean of the assumed differences is 3 units of AUC_s with a SD of 0.5.\nAn agnostic belief in a directional relationship: Both A>B and B>A are plausible, but uncertainty is high. The mean of the most plausible distribution is 0 with a SD of 3, i.e. a rather wide distribution, allowing effects in both directions.\n\nHere are three researchers and their prior beliefs:\nMichael holds strong prior beliefs that Typical exemplars exhibit less curvature than Atypical exemplars.\nNina is agnostic about the effect of condition on AUC_s.\nAs opposed to Michael, Jones holds strong prior beliefs that Typical exemplars exhibit MORE curvature than Atypical exemplars.\nSpecify priors for Michael, Nina, and Jones, and run models (set seed = 323) for all of these scenarios. Look at the results (maybe plot the posteriors if that helps you) and briefly describe how the priors affected the posteriors.\n\n\n\n\n\nShow solution\npriors_Michael <- c(\n   # Atypical > Typical\n   set_prior(\"normal(-3, 0.5)\", class = \"b\", coef = \"condition2\")\n)\n\npriors_Nina <- c(\n   # Atypical < Typical\n   set_prior(\"normal(0, 3)\", class = \"b\", coef = \"condition2\")\n)\n\npriors_Jones <- c(\n      set_prior(\"normal(3, 0.5)\", class = \"b\", coef = \"condition2\")\n)\n\n\n# model\nmodel5_Michael = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Michael\n  )\n\nmodel5_Nina = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Nina\n  )\n\nmodel5_Jones = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Jones\n  )\n\n# extract posteriors\n\n## Michael\nposteriors_Michael <- model5_Michael %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  mutate(model = \"Michael\")\n  \n\n## Nina\nposteriors_Nina <- model5_Nina %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\")  %>% \n  mutate(model = \"Nina\")\n\n## Jones\nposteriors_Jones <- model5_Jones %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  mutate(model = \"Jones\")\n\n# add to one df\nposteriors_all <- rbind(posteriors_Michael, posteriors_Nina, posteriors_Jones)\n\n# plot posteriors for all models\nggplot(posteriors_all, aes(x = posterior, y = parameter, fill = model, color = model)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95, alpha = 0.4) +\n    facet_grid(model~ .) +\n    # add axes titles\n    xlab(\"\\nAUC\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf, color = \"black\",\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-1.5, 0.5))\n\n#ANSWER: The agnostic Nina serves as a baseline. With a weakly informative prior, the likelihood dominates the posterior.\n# Michael has a strong preconception that Atypical exemplars elicit larger AUC values and in comparison to Jones, the posterior distribution is slightly shifted more toward negative values.\n# Jones has a strong preconception that Typical exemplars elicit larger AUC values and in comparison to Jones and Michael, the posterior distribution is slightly shifted AWAY from negative values.\n# Here, the priors, although encoding strong prior beliefs, have only little impact on the posteriors but do shift posteriors toward the priors."
  },
  {
    "objectID": "practice-sheets/00-preamble.html",
    "href": "practice-sheets/00-preamble.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nCode\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html",
    "href": "practice-sheets/02b-catPreds-tutorial.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\nGeneralized (non-)linear mixed effect models are a powerful statistical tool that gains increasing popularity for data analysis in cognitive science and many other disciplines. This tutorial will provide an overview of different categorical variable coding schemes used in mixed effect models. We will look at two example data sets from factorial-design experiments with categorical predictors and a continuous dependent variable which we will analyze using a Bayesian approach."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#dataset",
    "href": "practice-sheets/02b-catPreds-tutorial.html#dataset",
    "title": "Bayesian regression: theory & practice",
    "section": "Dataset",
    "text": "Dataset\nThe first part of this tutorial is based on a data set from an experiment by Winter and Grawunder (2012) You can get the dataset by running:\n\npoliteness_df <- faintr::politeness\n\n# get a look at the data set\nhead(politeness_df)\n\n# A tibble: 6 × 5\n  subject gender sentence context pitch\n  <chr>   <chr>  <chr>    <chr>   <dbl>\n1 F1      F      S1       pol      213.\n2 F1      F      S1       inf      204.\n3 F1      F      S2       pol      285.\n4 F1      F      S2       inf      260.\n5 F1      F      S3       pol      204.\n6 F1      F      S3       inf      287.\n\n\nThe data contains records of the voice pitch of speakers in different social contexts (polite and informal). They investigated whether the mean voice pitch differs across the factor gender of the speakers (F and M) and across the factor contexts - resulting in four different condition combinations (gender X context). Such a design is called factorial design and the single combinations are called design cells."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#explore-data-visually",
    "href": "practice-sheets/02b-catPreds-tutorial.html#explore-data-visually",
    "title": "Bayesian regression: theory & practice",
    "section": "Explore Data visually",
    "text": "Explore Data visually\nBefore we dive into any statistical analyses of our dataset it is helpful to get a rough idea of what the data looks like. For example, we can start by exploring the dataset visually.\n\n\n\n\n\nFurthermore, we can compute some basic statistics - e.g. the mean of the different design cells, before we turn to more complex linear models. We can also compute the overall mean pitch across all the conditions - the grand mean. These values will be helpful for a sanity check when interpreting the linear models later on.\n\ntibble_means <- politeness_df %>%\n  group_by(context, gender) %>%\n  summarize(mean = mean(pitch))\nhead(tibble_means)\n\n# A tibble: 4 × 3\n# Groups:   context [2]\n  context gender  mean\n  <chr>   <chr>  <dbl>\n1 inf     F       261.\n2 inf     M       144.\n3 pol     F       233.\n4 pol     M       133.\n\nmean(tibble_means$mean)\n\n[1] 192.8605"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nDummy coding, or treatment coding, is the default coding scheme used by R. Understanding the name ‘treatment coding’ helps understanding what this coding scheme does: imagine a medical experiment with a single control group (who obtain a placebo) and different experimental groups each of which gets a different treatment (e.g., different drugs), and where we want to compare each treatment group to the single, pivotal control group. Consequently, dummy coded variables are estimated by comparing all levels of the variable to a reference level. The intercept of a linear model containing dummy-coded variables is the mean of the reference level.\nOur variables only have two levels, so the effect of gender could be estimated by treating female as the reference level and estimating the effect of being male compared to the reference level – so basically estimating the difference in pitch it takes to “get from female to male”. Similarly, we can estimate the effect of context: the informal context can be treated as the reference level and the effect of politeness can be estimated against it. By default, the first level of a factor is treated as the reference level (for unordered factors that is the first string in alphanumeric order) - but principally, there is no difference as to which level should be used as the reference level. It makes sense to choose the level which is in some sense the ‘control’ in your experimental design.\nBecause R uses dummy-coding by default, we can look at the default numerical coding right away. The function contrasts() displays the contrast matrix for the respective variable:\n\ncontrasts(politeness_df$gender)\n\n  M\nF 0\nM 1\n\ncontrasts(politeness_df$context)\n\n    pol\ninf   0\npol   1\n\n\nBut if we wish to explicitly assign a dummy (treatment) coding to a variable, we may do so by a built-in R function:\n\ncontrasts(politeness_df$gender) <- contr.treatment(2) # insert the number of levels here\n# check\ncontrasts(politeness_df$gender)\n\n  2\nF 0\nM 1\n\n\nSo both variables \\(x_1\\) and \\(x_2\\) can take either the value 0 or 1 (because we dummy-code both categorical variables; see below for more). We already defined the referenc levels of the single variables, now we can define the overall reference level of our model (by combining the two individual reference levels) – it is the mean pitch of female speakers in informal contexts.\nHaving set all the basics, we can now turn to computing a linear model of the mean pitch as predicted by the factors gender and context:\n\n# here, we only use fixed effects\nlm.dummy.FE <- brm(\n  pitch ~ gender * context,\n  data = politeness_df,\n  cores = 4,\n  iter = 1000\n)\nlm.dummy.FE.coefs <- fixef(lm.dummy.FE)[,1] %>% as.numeric() # get the estimated coefficients\nsummary(lm.dummy.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender * context \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            260.65      7.61   245.50   275.60 1.00     1303     1387\ngender2             -116.08     10.69  -136.95   -95.03 1.00     1260     1324\ncontextpol           -27.34     10.65   -48.12    -6.76 1.01     1107     1322\ngender2:contextpol    15.76     15.03   -14.81    45.65 1.00     1038     1235\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.20      3.01    30.95    42.67 1.00     2007     1384\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow how do we interpret the estimated coefficients?\nLet us recall the regression equation that is hidden behind this output: \\[y = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*x_1x_2\\]\nIn order to help us interpret the output, R assigns string names to the estimated coefficients using the names we used in the generic formula. The (Intercept) corresponds to \\(\\beta_0\\), genderM corresponds to \\(\\beta_1\\), contextpol corresponds to \\(\\beta_2\\) and genderM:contextpol (the interaction term) to \\(\\beta_3\\).\nFurther, let us recall the numerical coding of our variables: for \\(x_1\\) (gender) a 0 means female, a 1 means male; for \\(x_2\\) (context) a 0 means informal, a 1 means polite. So the computed values are the estimates for conditions differing from the respective reference conditions - i.e. when the respective \\(x\\) is a 1.\nTo get an estimate of a certain design cell (\\(y_i\\)) - let’s start with the mean pitch of female speakers (0 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)) - we just insert the corresponding numeric values for the corresponding \\(x\\) and the estimated value for the corresponding \\(\\beta\\). Thus we get:\n\ny1 = lm.dummy.FE.coefs[1] + lm.dummy.FE.coefs[2]*0 +\n  lm.dummy.FE.coefs[3]*0 + lm.dummy.FE.coefs[4]*(0)\ny1\n\n[1] 260.6502\n\n\nHence, the mean pitch of female speakers in informal context corresponds to the intercept. As a sanity check, we can recall that for dummy coded variables the model intercept is just the mean of the reference cell (in our case, female speakers in informal contexts!).\nLet’s now calculate the mean pitch of male speakers (1 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)):\n\ny2 = lm.dummy.FE.coefs[1] + lm.dummy.FE.coefs[2]*1 +\n  lm.dummy.FE.coefs[3]*0 + lm.dummy.FE.coefs[4]*(1*0)\ny2\n\n[1] 144.574"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nAnother common coding scheme is the simple coding (also called contrast coding). Simple coded variables are also compared to a reference level (just like dummy-coded ones). However, the intercept of a simple coded model is the grand mean – the mean of all cells (i.e. the mean of female-informal & female-polite & male-informal & male-polite cells).\nGenerally, this kind of coding can be created by subtracting \\(1/k\\) from the dummy coding contrast matrix, where \\(k\\) is the number of levels a variable has (in our case, both have two). Hence, the reference level will always only have negative values in the contrast matrix. The general rule is that the contrasts within a column have to add up to 0. R does not provide a built-in function for simple coding, but we can easily create the respective matrix ourselves by subtracting \\(1/k\\) (i.e. 1/2) from the dummy coding matrix:\n\n# manual creation of contrasts\ncontr.matrix <- matrix( rep(0.5, 2))\ndummy.matrix <- contr.treatment(2)\ncontr.coding <- dummy.matrix - contr.matrix\n\n# we should duplicate the values to not overwrite previous contrasts\npoliteness_df <- politeness_df %>%\n  mutate(context_contr = context,\n         gender_contr = gender)\ncontrasts(politeness_df$context_contr) <- contr.coding\ncontrasts(politeness_df$gender_contr)  <- contr.coding\n\nHence now the gender is coded as -0.5 for female and 0.5 for male; context is coded as -0.5 for informal and 0.5 for polite.\nLet’s again look at our regression model:\n\nlm.contr.FE <- brm(\n  pitch ~ gender_contr * context_contr,\n  data = politeness_df,\n  cores = 4,\n  iter =  1000\n)\nlm.contr.FE.coefs <- fixef(lm.contr.FE)[,1] %>% as.numeric() # get vector of estimated coefficients\nsummary(lm.contr.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender_contr * context_contr \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                      192.86      4.02   184.78   200.80 1.00     2241\ngender_contr2                 -108.46      7.84  -123.92   -92.84 1.00     2717\ncontext_contr2                 -19.81      8.03   -35.46    -3.50 1.00     2644\ngender_contr2:context_contr2    15.81     15.42   -14.10    45.21 1.00     1991\n                             Tail_ESS\nIntercept                        1313\ngender_contr2                    1560\ncontext_contr2                   1572\ngender_contr2:context_contr2     1599\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.23      2.98    31.11    43.02 1.00     2479     1387\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn order to compute the mean pitch of a specific cell, we proceed just as with dummy-coded variables and insert the respective estimates and values for \\(x\\). Let us start with female speakers (\\(x_1\\) is -0.5) in informal contexts (\\(x_2\\) is -0.5):\n\ny1 = lm.contr.FE.coefs[1] + lm.contr.FE.coefs[2]*(-0.5) + lm.contr.FE.coefs[3]*(-0.5) + lm.contr.FE.coefs[4]*(-0.5)*(-0.5)\ny1\n\n[1] 260.9499\n\n\nWe get the same result as before (as we should - the estimates should not depend on a coding scheme but only on the data). As a sanity check, we can again look at the intercept – it matches the grand mean we computed in the beginning of this tutorial – as it should.\nYour turn! Compute the pitch means for the other three conditions.\n\n# your code here"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nDeviation coding (also called sum coding) is the most popular coding scheme and is often considered the best choice to get a clear picture of presence (or absence) of an effect and a clear random effects interpretation.\nIt is slightly different from the previous schemes. It compares the mean of the predicted variable for a specific condition to the grand mean. So the estimates do not tell you the difference between the reference level and another level anymore. The intercept of linear models with sum coded variables is the grand mean.\nR has a built-in function for creating sum coded variables:\n\n# again create a new variable\npoliteness_df %>%\n  mutate(context_dev = context,\n         gender_dev = gender) -> politeness_df\ncontrasts(politeness_df$context_dev) <- contr.sum(2) # insert number of levels\ncontrasts(politeness_df$gender_dev)  <- contr.sum(2)\n\nNow the gender is coded as s 1 for female and -1 for male; context is coded as 1 for informal and -1 for polite.\nBelow we fit a model with the sum-coded variables:\n\nlm.dev.FE <- brm(pitch ~ context_dev * gender_dev,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\nlm.dev.FE.coefs <- fixef(lm.dev.FE)[,1] %>% as.numeric()\nsummary(lm.dev.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_dev * gender_dev \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  193.01      3.97   185.07   200.60 1.00     2369\ncontext_dev1                 9.84      4.01     2.12    17.51 1.00     2272\ngender_dev1                 54.12      4.17    45.89    62.22 1.00     2287\ncontext_dev1:gender_dev1     3.80      3.90    -3.60    11.33 1.00     2828\n                         Tail_ESS\nIntercept                    1449\ncontext_dev1                 1587\ngender_dev1                  1471\ncontext_dev1:gender_dev1     1533\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.16      2.98    31.03    42.74 1.00     2389     1443\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe coefficients denote now the difference between the grand mean (i.e. intercept) and the mean of the respective condition.\nWe apply the same idea to estimate the pitch means for specific cases: E.g. for female speakers in informal contexts we do:\n\ny1 = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2]*1 + lm.dev.FE.coefs[3]*1 + lm.dev.FE.coefs[4]*1*1\ny1\n\n[1] 260.7746\n\n\nSince the intercept is now the grand mean and not a specific reference level, let us think about the interpretation of the single estimates. The estimate of e.g. the context effect now denotes the value by which the mean pitch in informal (estimate * 1, remember our coding!) or polite contexts (estimate * -1) differs from the grand mean. So if we wish to calculate the mean pitch in polite contexts (across genders), we would do:\n\nyPol = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2] * (-1)\nyPol\n\n[1] 183.1742\n\n\nThis means that the single estimates are in some sense ‘independent’ of each other (in contrast to e.g. dummy-coded variables where the estimates are bound to the reference levels of two variables) and give us insight if a specific factor is credibly different from 0. Similarly, if we wish to calculate the mean pitch of male speakers, we would calculate:\n\nyM = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[3] * (-1)\nyM\n\n[1] 138.89"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nIn this coding scheme, a level of a variable is compared to its subsequent levels. In our dataset, e.g. for gender the level female is compared to the subsequent level male.\nGenerally, to create such a coding, in order to compare the first level to the subsequent levels you would assign \\((k-1)/k\\) to the first level and \\(-1/k\\) to all subsequent levels where \\(k\\) is the total number of levels. To compare the second level to the subsequent levels you would assign 0 to the first level, \\((i-1)/i\\) to the second level and \\(-i/1\\) to all subsequent levels where \\(i = k-1\\) and so on. The difference of this coding scheme to previous ones is more clear for variales with >2 levels (see below). The intercept of a linear model corresponds to the grand mean.\nR does not have a built-in function for standard Helmert coding, so we do it manually:\n\n# with politeness data\nhelm.matrix <- matrix(c(0.5, -0.5))\npoliteness_df <-\n politeness_df %>%\n mutate(gender_helm = gender,\n         context_helm = context)\ncontrasts(politeness_df$gender_helm)  <- helm.matrix\ncontrasts(politeness_df$context_helm) <- helm.matrix\n\nThe linear model looks like this:\n\nlm.helmert.FE <- brm(pitch ~ context_helm * gender_helm,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\nlm.helmert.FE.coefs <- fixef(lm.helmert.FE)[,1] %>% as.numeric()\nsummary(lm.helmert.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_helm * gender_helm \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    192.82      4.07   184.94   200.75 1.00     2174\ncontext_helm1                 19.61      8.11     3.87    35.57 1.00     2796\ngender_helm1                 108.34      8.18    91.97   124.57 1.01     1884\ncontext_helm1:gender_helm1    15.97     16.13   -15.69    46.93 1.00     2498\n                           Tail_ESS\nIntercept                       888\ncontext_helm1                  1573\ngender_helm1                   1180\ncontext_helm1:gender_helm1     1586\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.18      2.96    31.10    42.62 1.00     2009     1354\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe contrast estimate for the first level and the remaining levels is calculated by taking the mean of the dependent variable for the first level and subtracting the mean of the dependent variable for the remaining levels (in our case, just the mean of the second level). In other words, if we look at the context coefficient it denotes the difference between the mean of the polite and informal context means."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#mixing-coding-schemes",
    "href": "practice-sheets/02b-catPreds-tutorial.html#mixing-coding-schemes",
    "title": "Bayesian regression: theory & practice",
    "section": "Mixing Coding Schemes",
    "text": "Mixing Coding Schemes\nIf you have several categorical predictor variables, it is also possible (and often useful!) to use different coding schemes for the different variables. It might, for example, make sense to use dummy coding for a variable which has a control and a treatment condition, and to use e.g. simple coding for a variable which has two ‘equal’ levels.\nFor example, we could use dummy-coding for context and simple-coding for gender.\nWhen you mix coding schemes or define your own schemes there might be no pre-defined answers to questions as to what the sigle coefficients or the intercept mean. But knowing how the different schemes work, you can easily find this out!\nLet us explore how the interpretation of the model changes if we mix coding schemes:\n\nlm.mixedCode.FE <- brm(pitch ~ context * gender_contr,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\nlm.mixedCode.FE.coefs <- fixef(lm.mixedCode.FE)[,1] %>% as.numeric()\nsummary(lm.mixedCode.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context * gender_contr \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  202.63      5.57   191.72   214.02 1.00     2133\ncontextpol                 -19.42      8.04   -34.75    -3.84 1.00     2025\ngender_contr2             -116.21     10.98  -138.44   -94.98 1.00     1803\ncontextpol:gender_contr2    15.92     15.95   -16.77    47.25 1.00     1724\n                         Tail_ESS\nIntercept                    1456\ncontextpol                   1380\ngender_contr2                1410\ncontextpol:gender_contr2     1290\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.32      2.99    30.91    42.64 1.00     2068     1509\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nGenerally, the interpretation is just the combination of what we have learned about the individual coding schemes. Recall that the intercept of a dummy-coded model is the mean of the reference level – since we dummy-coded context, the refernce level would be informal context. But it is not the intercept yet! We have the second predictor in our model – the simple coded gender. In simple coded models the intercept is the mean across the levels of the variable. Now the intercept of our model with the two different predictors is the mean pitch in informal contexts - across genders.\nFollowing this logic, the context estimate denotes the difference between the informal and polite contexts - still across genders. The gender estimate denoted the difference between the mean pitch and female speakers if multiplied by the value -0.5 (recall our coding above); and the mean pitch and male speakers if multiplied by the value 0.5."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nGenerally, the coding schemes work in the very same way independently of the number of levels. The only difference to our old data set is that now we need two numeric variables coding the contrasts between the levels of one variable. If we look at the default (dummy) coding of e.g. the variable List we see a contrast matrix with two columns, each denoting the comparisons between two levels:\n\ncontrasts(latinsquare$SOA)\n\n       medium short\nlong        0     0\nmedium      1     0\nshort       0     1\n\ncontrasts(latinsquare_df$List)\n\n   L2 L3\nL1  0  0\nL2  1  0\nL3  0  1\n\n\nSo now the recoding of the categorical variable takes two numeric variables: e.g. \\(x_{1_2}\\) and \\(x_{1_3}\\), where both can take the values 0 or 1; the single levels are denoted by the combination of the two numeric variables. Again there is a reference level - List 1 - described by \\(x_{1_2}\\) and \\(x_{1_3}\\) being 0. \\(x_{1_2}\\) being 1 describes the difference between the reference level and List 2; \\(x_{1_3}\\) being 1 describes the difference between the reference level and List 3. Correspondingly, there is and individual \\(\\beta\\) for each numeric variable estimated in the regression model. The coding of the SOA factor works just the same way. The interactions between specific levels are described by combining the respective numeric variables \\(x\\). So the model we are fitting is described by:\n\\[y = \\beta_0 + \\beta_1 * x_{1_2} + \\beta_2 * x_{1_3} + \\beta_3 * x_{2_2} + \\beta_4 * x{2_3} + \\beta_5 * x_{1_2}x_{2_2} + \\beta_6 * x_{1_3}x_{2_2} + \\beta_7 * x_{1_2}x_{2_3}  + \\beta8 * x_{1_3}x_{2_3}\\]\n\nlm3.dummy.FE <- brm(RT ~ List * SOA,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.dummy.FE.coefs <- fixef(lm3.dummy.FE)[,1] %>% as.numeric()\nsummary(lm3.dummy.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          529.41     11.73   506.51   552.16 1.00      915     1162\nListL2              14.02     16.74   -18.75    47.68 1.00      910     1250\nListL3              -0.07     16.57   -32.54    33.01 1.00      989     1259\nSOAmedium            7.40     17.11   -25.90    42.15 1.00     1010      831\nSOAshort            14.88     16.61   -16.45    47.40 1.00      883     1004\nListL2:SOAmedium     5.42     23.66   -40.75    50.42 1.00      949     1000\nListL3:SOAmedium   -21.47     23.93   -69.29    23.76 1.00     1063     1010\nListL2:SOAshort    -24.15     23.46   -69.93    21.44 1.00      875     1343\nListL3:SOAshort    -22.34     23.53   -69.96    24.54 1.00      974     1237\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.57      2.84    41.58    52.89 1.00     1960     1352\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince both predictors are dummy-coded the intercept represents the reference level - the mean RT for List 1 and a long SOA. Following the same procedure as for the two-level variables you could calculate the estimated mean RTs for specific conditions."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nSimple coding only slightly differs from dummy-coding – the intercept of the model is the grand mean, not the mean RT of the reference level. Otherwise, the coefficients still denote the difference between the reference level and other specific levels.\n\nlatinsquare_df %>%\n  mutate(List_contr = factor(List),\n         SOA_contr = factor(SOA)) -> latinsquare_df\ndummy.matrix3 <- contr.treatment(3)\ncontr.matrix3 <- matrix(c(1/3, 1/3, 1/3, 1/3, 1/3, 1/3), ncol=2)\ncontrasts(latinsquare_df$List_contr) <- dummy.matrix3 - contr.matrix3\ncontrasts(latinsquare_df$SOA_contr) <-  dummy.matrix3 - contr.matrix3\n\n\nlm3.simple.FE <- brm(RT ~ List_contr * SOA_contr,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.simple.FE.coefs <- fixef(lm3.simple.FE)[,1] %>% as.numeric()\nsummary(lm3.simple.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_contr * SOA_contr \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.58      3.85   527.12   542.06 1.00     2726\nList_contr2                8.10      9.67   -11.50    26.86 1.00     1719\nList_contr3              -14.54      9.41   -33.16     3.39 1.00     1840\nSOA_contr2                 2.15      9.53   -16.60    20.61 1.00     2202\nSOA_contr3                -0.54      9.50   -19.30    17.80 1.00     2035\nList_contr2:SOA_contr2     5.43     23.64   -42.98    50.88 1.00     1824\nList_contr3:SOA_contr2   -22.00     22.53   -67.04    22.11 1.00     1769\nList_contr2:SOA_contr3   -23.24     23.44   -67.79    22.87 1.00     1600\nList_contr3:SOA_contr3   -21.75     22.32   -65.26    22.21 1.00     1724\n                       Tail_ESS\nIntercept                  1418\nList_contr2                1398\nList_contr3                1632\nSOA_contr2                 1669\nSOA_contr3                 1692\nList_contr2:SOA_contr2     1590\nList_contr3:SOA_contr2     1708\nList_contr2:SOA_contr3     1310\nList_contr3:SOA_contr3     1533\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.61      2.81    41.25    52.42 1.00     2543     1393\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nWith increasing number of levels within the factors the complexity and messiness of interpreting the differences between levels against each other increases considerably. Hence it might make a lot of sense to use the deviation coding scheme which provides estimates of effects comapred to the grand mean. We again can use the R built-in function to assign deviation coding to our three-level factors:\n\nlatinsquare_df %>%\n  mutate(List_dev = List,\n         SOA_dev = SOA) -> latinsquare_df\ncontrasts(latinsquare_df$List_dev) <- contr.sum(3) # insert number of levels\ncontrasts(latinsquare_df$SOA_dev) <- contr.sum(3)\n\nFor e.g. SOA our numeric variables now denote the effect of long SOA compared to the grand mean when \\(x_{2_2}\\) is a 1 and \\(x_{2_3}\\) is a 0; they denote the effect of medium SOA compared to the grand mean when \\(x_{2_2}\\) is a 0 and \\(x_{2_3}\\) is a 1; the effect of short SOA is never compared to the grand mean since it is always assigned a -1.\n\nlm3.dev.FE <- brm(RT ~ List_dev * SOA_dev,\n                   data = latinsquare_df,\n                 cores = 4,\n                 iter = 1000)\nlm3.dev.FE.coefs <- fixef(lm3.dev.FE)[,1] %>% as.numeric()\nsummary(lm3.dev.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_dev * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            534.66      3.71   527.41   542.17 1.00     3266     1547\nList_dev1              2.37      5.47    -8.13    12.53 1.00     1877     1507\nList_dev2             10.04      5.48    -1.07    20.78 1.00     1595     1373\nSOA_dev1              -0.60      5.55   -11.87    10.21 1.00     1904     1546\nSOA_dev2               1.62      5.45    -9.15    12.51 1.00     1832     1518\nList_dev1:SOA_dev1    -6.69      7.72   -22.45     8.03 1.00     1502     1649\nList_dev2:SOA_dev1    -1.04      7.73   -16.54    13.67 1.00     1363     1562\nList_dev1:SOA_dev2    -1.39      7.97   -16.82    14.26 1.00     1652     1619\nList_dev2:SOA_dev2    10.14      7.77    -5.34    24.99 1.00     1313     1593\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.53      2.87    41.37    52.65 1.00     2179     1290\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThink about what the single estimates mean!"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nFor recap: In this coding scheme, a level of a variable is compared to its subsequent levels. What does this mean for the three-level factors?\n\nlatinsquare_df %>%\n  mutate(List_helm = List,\n         SOA_helm = SOA) -> latinsquare_df\nhelm.matrix3 <- matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2 ), ncol = 2)\ncontrasts(latinsquare_df$List_helm) <- helm.matrix3\ncontrasts(latinsquare_df$SOA_helm) <- helm.matrix3\n\n\nlm3.helm.FE <- brm(RT ~ List_helm * SOA_helm,\n                   data = latinsquare_df,\n                  cores = 4,\n                  iter = 1000)\nlm3.helm.FE.coefs <- fixef(lm3.helm.FE)[,1] %>% as.numeric()\nsummary(lm3.helm.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_helm * SOA_helm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept              534.51      3.93   526.64   542.37 1.00     4074\nList_helm1               3.38      8.02   -12.30    19.44 1.00     4192\nList_helm2              22.16      9.49     3.94    40.73 1.00     4069\nSOA_helm1               -0.86      8.24   -17.22    15.08 1.01     3624\nSOA_helm2                2.86      9.65   -16.18    21.74 1.02     5085\nList_helm1:SOA_helm1   -15.50     18.43   -53.26    20.62 1.00     3987\nList_helm2:SOA_helm1   -12.56     20.56   -53.59    28.67 1.00     4623\nList_helm1:SOA_helm2   -14.74     20.24   -55.00    24.93 1.00     3843\nList_helm2:SOA_helm2    28.32     23.99   -20.64    74.02 1.00     4645\n                     Tail_ESS\nIntercept                1396\nList_helm1               1579\nList_helm2               1308\nSOA_helm1                1297\nSOA_helm2                1278\nList_helm1:SOA_helm1     1410\nList_helm2:SOA_helm1     1195\nList_helm1:SOA_helm2     1717\nList_helm2:SOA_helm2     1293\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.59      2.76    41.68    52.16 1.00     3385     1527\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe main effect estimates denote the differences between the mean of List1 and the mean of (List2 + List3); and between the mean of List2 and the mean of List3. Respectively, they denote the differences between the mean of SOA long and the mean of (medium + short); and between the mean of SOA medium and the mean of short. The intercept is the grand mean."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#reverse-helmert-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#reverse-helmert-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Reverse Helmert Coding",
    "text": "Reverse Helmert Coding\nThe reverse Helmert coding scheme (also called difference coding) is quite similar to the Helmert coding, but compares the mean of a level to its previous levels. Since we basically reverse the coding we used in the previous scheme, we also ‘reverse’ the contrast matrix to create such a coding.\n\nlatinsquare_df %>%\n  mutate(List_rhelm = List,\n         SOA_rhelm = SOA) -> latinsquare_df\nrhelm.matrix3 <- matrix(c(-1/2, 1/2, 0, -1/3, -1/3, 2/3 ), ncol = 2)\ncontrasts(latinsquare_df$List_rhelm) <- rhelm.matrix3\ncontrasts(latinsquare_df$SOA_rhelm) <- rhelm.matrix3\n\n\nlm3.rhelm.FE <- brm(RT ~ List_rhelm * SOA_rhelm,\n                   data = latinsquare_df,\n                   iter = 1000,\n                   cores = 4)\nlm3.rhelm.FE.coefs <- fixef(lm3.rhelm.FE)[,1] %>% as.numeric()\nsummary(lm3.rhelm.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_rhelm * SOA_rhelm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.54      3.96   526.95   542.12 1.00     4210\nList_rhelm1                8.09      9.67   -10.90    28.55 1.00     3179\nList_rhelm2              -18.47      8.15   -34.67    -2.69 1.00     4683\nSOA_rhelm1                 1.98      9.50   -17.06    20.93 1.00     5128\nSOA_rhelm2                -1.64      8.25   -17.85    14.19 1.00     5260\nList_rhelm1:SOA_rhelm1     6.12     23.63   -39.96    52.64 1.00     5240\nList_rhelm2:SOA_rhelm1   -24.94     19.81   -63.50    13.55 1.00     3416\nList_rhelm1:SOA_rhelm2   -25.97     20.70   -67.95    16.11 1.00     3921\nList_rhelm2:SOA_rhelm2     2.11     16.47   -29.50    33.24 1.00     3927\n                       Tail_ESS\nIntercept                  1443\nList_rhelm1                1445\nList_rhelm2                1760\nSOA_rhelm1                 1541\nSOA_rhelm2                 1597\nList_rhelm1:SOA_rhelm1     1475\nList_rhelm2:SOA_rhelm1     1197\nList_rhelm1:SOA_rhelm2     1296\nList_rhelm2:SOA_rhelm2     1626\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.48      2.92    41.17    52.71 1.00     3242     1402\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn our example, the first estimate denotes the difference between the mean of List2 and the mean of List1; the second - the difference between the mean of List3 and the mean of (List1 + List2). The main effects of SOA can be interpreted similarly."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Mixed Schemes: Dummy and Deviation Coding",
    "text": "Mixed Schemes: Dummy and Deviation Coding\nJust like with two-level factors, we might wish to use different coding schemes for different predictors. It might make sense to use dummy coding for a variable which has a control and two different treatment conditions, and to use deviation coding for a variable which has ‘equal’ levels.\nFor example, we could use dummy-coding for List and deviation-coding for SOA.\n\nlm3.mixedCode.FE <- brm(RT ~ List * SOA_dev,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.mixedCode.FE.coefs <- fixef(lm3.mixedCode.FE)[,1] %>% as.numeric()\nsummary(lm3.mixedCode.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         536.77      6.87   523.24   550.06 1.00     1808     1523\nListL2              7.87      9.78   -11.64    27.11 1.00     1780     1491\nListL3            -14.62      9.83   -33.87     4.65 1.00     1758     1254\nSOA_dev1           -7.55      9.56   -26.60    11.05 1.00     1095     1412\nSOA_dev2            0.27     10.03   -19.12    19.94 1.00      991     1439\nListL2:SOA_dev1     6.12     13.60   -20.68    32.20 1.00     1247     1495\nListL3:SOA_dev1    14.67     13.61   -12.76    41.41 1.00     1270     1356\nListL2:SOA_dev2    11.06     14.08   -17.87    38.54 1.00     1235     1321\nListL3:SOA_dev2    -7.28     14.17   -33.62    20.86 1.00     1156     1495\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.55      2.86    41.47    52.59 1.00     2001     1343\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nTowards the end of this tutorial, the main take-away is this: when you look at the estimates of (any) model, you could ask yourself a couple of questions like these to make sure you understand what was calculated:\n\nWhat does the intercept represent?\nWhat do the single estimates mean?\nWhat do they tell me about my hypotheses?\n\nOf course, you will also encounter experimental designs which use a two-level and a three-level categorical predictors – but the conceptual basics regarding how to choose the contrasts and how to interpret linear models are the same."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html",
    "href": "practice-sheets/01b-simple-regression.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#data-wrangling",
    "href": "practice-sheets/01b-simple-regression.html#data-wrangling",
    "title": "Bayesian regression: theory & practice",
    "section": "Data wrangling",
    "text": "Data wrangling\n\n# aggregate\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(\n            AUC = median(AUC, na.rm = TRUE),\n            MAD = median(MAD, na.rm = TRUE)) \n  \n# let's have a look\nhead(dolphin_agg)\n\n# A tibble: 6 × 3\n  subject_id     AUC    MAD\n       <dbl>   <dbl>  <dbl>\n1       1001  55200. 111.  \n2       1002  59596.  87.9 \n3       1003 -17772  -34.1 \n4       1004  -3600.  -3.83\n5       1005  54054   95.0 \n6       1006  60396. 155."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#visual-assessment",
    "href": "practice-sheets/01b-simple-regression.html#visual-assessment",
    "title": "Bayesian regression: theory & practice",
    "section": "Visual assessment",
    "text": "Visual assessment\nBefore we start thinking about statistical inference, we always want to get a feel for the data visually. You basically always want to plot the data.\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_point(size = 3, alpha = 0.3) \n\n\n\n\nThis graph displays the distribution of AUC and MAD values.\nLooking at the plot, we can see that there is a strong relationship between AUC and MAD. And that makes a lot of sense. The larger the cursor strives toward the competitor, the larger is the overall area under the curve. Heureka! Our hypothesis is confirmed.\nBut wait! As Bayesians, we would like to translate the data into an expression of evidence: do the data provide evidence for our research hypotheses? Also, notice that there is some variability. We want precise estimates of potential effects. We also want a measure of how certain we can be about these estimates."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#bayesian-linear-regression-with-brms",
    "href": "practice-sheets/01b-simple-regression.html#bayesian-linear-regression-with-brms",
    "title": "Bayesian regression: theory & practice",
    "section": "Bayesian linear regression with brms",
    "text": "Bayesian linear regression with brms\nThe brms package allows us to run Bayesian regression models, both simple and rather complex. It uses a sampling method, so its output will be vectors of (corellated) samples from the posterior distribution of the model’s parameters. (We will learn how this sampling method works later).\nSo, to quantify evidence and uncertainty with posterior samples, let’s run a simple linear regression model using brms. We use the R notation that some of your might already be familiar with when using lm(). We specify a formula in which AUC is predicted by MAD.\nAUC ~ MAD\nWhen you run this code, the brms package generates Stan code and runs the Stan program in the background. Stan code is executed in C++, and the model will be ‘compiled’ (you get information about this in the console output). We will learn later what this compilation does (spoiler: it computes gradients for all stochastic nodes in the model). The only thing that is relevant for you at the moment is this: This compilation can take quite a while (especially for complex models) before anything happens.\n\n# specify the model \nmodel1 = brm(\n  # model formula\n  AUC ~ MAD, \n  # data\n  data = dolphin_agg\n  )\n\n\nsummary(model1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: AUC ~ MAD \n   Data: dolphin_agg (Number of observations: 108) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   502.63   1859.64 -3124.56  4193.83 1.00     4150     2515\nMAD         455.19     16.63   421.55   487.69 1.00     4024     2919\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma 17198.21   1181.43 15021.52 19759.05 1.00     4213     2711\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe output of such a model looks very familiar if you have worked with lm() before. We want to look at what is here called “Population-Level Effects”, which is a small table in this case. The first column contains the names of our coefficients; the Estimate column gives us the posterior mean of these coefficients; the Est.Error give us the standard error; the l-95%and u-95% give us the lower and upper limit of the 95% Credible Interval (henceforth CrI). The column Rhat (R^) which is a diagnostic of chain convergence and should not diverge much from 1 (rule of thumb: should by <1.1). Again, more about that later. The Bulk_ESS and Tail_ESS columns give us numbers of “useful” samples. This number should be sufficiently high. If its not, brms will give you a convenient warning (more about that later, so don’t worry for now). If that happens, you need to increase the chains and / or the number of iterations in order to increase the overall number of samples (again, don’t worry for now).\nIf we need the main summary output in a tidy tibble format, we can use this function from the tidybayes package:\n\ntidybayes::summarise_draws(model1)\n\n# A tibble: 5 × 10\n  variable    mean  median      sd     mad      q5     q95  rhat ess_b…¹ ess_t…²\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl>   <dbl>\n1 b_Inter…   503.    488.  1.86e+3 1.85e+3 -2523.   3630.   1.00   4150.   2515.\n2 b_MAD      455.    455.  1.66e+1 1.65e+1   427.    483.   1.00   4024.   2919.\n3 sigma    17198.  17147.  1.18e+3 1.12e+3 15351.  19325.   1.00   4213.   2711.\n4 lprior     -22.3   -22.3 2.92e-2 2.83e-2   -22.4   -22.3  1.00   4031.   2241.\n5 lp__     -1218.  -1218.  1.23e+0 1.02e+0 -1221.  -1217.   1.00   2054.   2535.\n# … with abbreviated variable names ¹​ess_bulk, ²​ess_tail\n\n\nThe model output suggests that the posterior mean of the Intercept is around 503 . The coefficient for MAD is estimated to be about 455.\nTo see how good a fit this is, we should manually draw this line into the graph from above.\n\n# extract model parameters:\nmodel_intercept <- summary(model1)$fixed[1,1]\nmodel_slope <- summary(model1)$fixed[2,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size  = 1) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\nLooking at the graph, it does make sense, right? The red line seems to capture the main trend pretty well.\nNow is there a relationship between AUC and MAD? What would it mean if there was no relationship between these two measures? Well no relationship would mean a slope of 0. How would that look like?\n\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size = 1) +\n  geom_abline(intercept = model_intercept, slope = 0, color = project_colors[3], size = 1, lty = \"dashed\") +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\nThese lines look quite different indeed. But Bayesian data analysis does not give us only one single line. It gives us infinitely many lines, weighted by plausibility. Let’s explore this universe of weighted predictions."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#extracting-posterior-distributions-and-plotting-them",
    "href": "practice-sheets/01b-simple-regression.html#extracting-posterior-distributions-and-plotting-them",
    "title": "Bayesian regression: theory & practice",
    "section": "Extracting posterior distributions and plotting them",
    "text": "Extracting posterior distributions and plotting them\nWe can interpret and visualize our coefficients immediately. We can create a data frame with all posterior samples for each parameter and plot those distributions for all coefficients. Let’s first see what coefficients there are with the get_variables() function from the tidybayes package.\n\n# inspect parameters\ntidybayes::get_variables(model1)\n\n [1] \"b_Intercept\"   \"b_MAD\"         \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\nEverything that is preceded by a b_ is a population level coefficients, i.e. our predictors. Now let’s wrangle this data frame to get what we need. You don’t have to entirely understand the following code, but make sure you understand it well enough to recycle it later on.\n\n# wrangle data frame\nposteriors1 <- model1 |>\n  tidybayes::spread_draws(b_MAD, b_Intercept) |>\n  select(b_MAD, b_Intercept)\n\nposteriors1\n\n# A tibble: 4,000 × 2\n   b_MAD b_Intercept\n   <dbl>       <dbl>\n 1  462.        588.\n 2  482.      -3942.\n 3  429.       4903.\n 4  438.        974.\n 5  455.       -714.\n 6  450.       1464.\n 7  466.      -1609.\n 8  459.        589.\n 9  466.      -2831.\n10  440.       1408.\n# … with 3,990 more rows\n\n\nNow that we know how to extract posterior samples, let’s actually take a bunch of these samples and plot them as lines into our scatter plot from above. In this code chunk we generate a subsample of 100 parameter pairs.\n\n# wrangle data frame\nposteriors2 <- model1 |>\n  # parameter 'ndraws' requests 100 random subsamples\n  tidybayes::spread_draws(b_MAD, b_Intercept, ndraws = 100) |>\n  select(b_MAD, b_Intercept)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(data = posteriors2,\n              aes(intercept = b_Intercept, slope = b_MAD), \n              color = project_colors[2], size  = 0.1, alpha = 0.4) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1]) +\n  theme_aida()\n\n\n\n\n\n\n\n\nGiven our model, assumptions and data, these are 100 plausible regression lines. As you can see they are very similar.\nUsing this pipeline we can also calculate the mean of the posteriors and any kind of Credible Interval (CrI). We first extract the posterior and bring them into a tidy form. Let’s only look at the coefficient for MAD here.\n\nposteriors3 <- model1 |>\n   # use the gather_draws() function for \"long data\"\n   tidybayes::gather_draws(b_MAD) |> \n   # change names of columns\n   rename(parameter = .variable,\n          posterior = .value) |> \n   # select only those columns that are relevant\n   select(parameter, posterior)\n\nhead(posteriors3)\n\n# A tibble: 6 × 2\n# Groups:   parameter [1]\n  parameter posterior\n  <chr>         <dbl>\n1 b_MAD          462.\n2 b_MAD          482.\n3 b_MAD          429.\n4 b_MAD          438.\n5 b_MAD          455.\n6 b_MAD          450.\n\n\nAnd then calculate the mean, the lower and the upper bound of a 90% CrI, using the function tidybayes::hdi().\n\n# get posteriors for the relevant coefficients\nposteriors3_agg <- posteriors3 |> \n  group_by(parameter) |> \n  summarise(\n    `90lowerCrI`   = tidybayes::hdi(posterior, credMass = 0.90)[1],\n    mean_posterior = mean(posterior),\n    `90higherCrI`  = tidybayes::hdi(posterior, credMass = 0.90)[2])\n\nposteriors3_agg \n\n# A tibble: 1 × 4\n  parameter `90lowerCrI` mean_posterior `90higherCrI`\n  <chr>            <dbl>          <dbl>         <dbl>\n1 b_MAD             422.           455.          488.\n\n\nNow we use this newly created data frame to plot the posterior distributions of all population-level coefficients. Again, we use our new best friend, the tidybayes package which offers some sweet extensions to ggplot’s geom_ family of functions. We also add a reference point to compare the posteriors against. A common and reasonable reference point is 0. Remember a slope coefficient of zero would correspond to a flat regression line.\n\n# plot the regression coefficients\nposteriors1 |> \n  pivot_longer(cols = everything(), names_to = \"parameter\", values_to = \"posterior\") |> \n  ggplot(aes(x = posterior, y = parameter, fill = parameter)) + \n    # plot density w/ 90% credible interval\n    tidybayes::stat_halfeye(.width = 0.9) +\n    # add axes titles\n    xlab(\"\") +\n    ylab(\"\") +\n    # adjust the x-axis \n    scale_x_continuous(limits = c(-100,600)) +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    theme(legend.position=\"none\")\n\n\n\nposteriors3_agg[1,2]\n\n# A tibble: 1 × 1\n  `90lowerCrI`\n         <dbl>\n1         422.\n\n\nHere you see density plots for our critical coefficients of the model. We care mostly about the slope coefficient (b_MAD) (the posterior of which is shown in red). Values between about 421.5062123 and about 487.6287135 are plausible (at the 90% level) and they are indicated by the thick black line in the density plot for this coefficient. The mean of the distribution is indicated by the thick black dot.\nThat’s helpful because we can relate this distribution to relevant values, for example the value 0 (dashed line). If you look at the coefficient, you can see that the posterior distribution does not include the value zero or any small-ish “Region of Practical Equivalence” around it. In fact, the posterior is really far away from zero. Thus, if we believe in the data and the model, we can be very certain that this coefficient is not zero. In other words, we would be very certain that there is a positive relationship between AUC and MAD (and in turn that ‘no relationship’ is not a very plausible scenario).\nThe brms package allows us to quickly evaluate how many posterior samples fall into a certain value range. Just for fun, let’s calculate the amount of posterior samples that are larger than 450. The following code chunk does this for us:\n\nhypothesis(model1, 'MAD > 450')\n\nHypothesis Tests for class b:\n       Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob\n1 (MAD)-(450) > 0     5.19     16.63   -22.67    33.04       1.65      0.62\n  Star\n1     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe results tell us that more than 60% of all posterior samples are larger than 450. It also tells us the evidence ratio (more on this later), which is the odds of the hypothesis in question (here ’MAD > 450)."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#exercises-for-simple-regression-modeling",
    "href": "practice-sheets/01b-simple-regression.html#exercises-for-simple-regression-modeling",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises for simple regression modeling",
    "text": "Exercises for simple regression modeling\n\n\n\n\n\n\nExercise 1a\nMassage the data and create a new dataset that contains only correct responses and only the mean values of the RT and the AUC measurement for each participant (subject_id). Print out the head of the dataset.\n\n\n\n\n\nShow solution\n# aggregate\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(AUC = mean(AUC, na.rm = TRUE),\n            RT = mean(RT, na.rm = TRUE))\n  \n# let's have a look\nhead(dolphin_agg)\n\n\nWe know from the previous exercises (walkthrough) that the area-under-the-curve (AUC) is related to the maximum absolute deviation (MAD). But what about reaction times (RTs)? Isn’t it plausible that RTs are also related to AUC? The further I curve away from the target with the cursor, the longer it takes me to arrive at the target, right?\n\n\n\n\n\n\nExercise 1b\nPlot the relationship between RT and AUC in a scatterplot.\n\n\n\n\n\nShow solution\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = AUC)) + \n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\nExercise 1c\nRun a linear regression using brms. AUC is the dependent variable (i.e. the measure) and RT is the independent variables (i.e. the predictor). The formula writes: AUC ~ RT\n\n\n\n\n\nShow solution\n# specify the model \nmodel1 <- brm(\n  # model formula\n  AUC ~ RT, \n  # data\n  data = dolphin_agg\n  )\n\nsummary(model1)\n\n\n\n\n\n\n\n\nExercise 1d\nLook at the model output. Think of it in terms of a line in the scatterplot from (1b). Where does the regression line cross the y-axis, what is the slope of the line? Draw a scatterplot of AUC against RT and add the predicted values as a line.\n\n\n\n\n\nShow solution\n# extract model parameters:\nmodel_intercept <- summary(model1)$fixed[1,1]\nmodel_slope <- summary(model1)$fixed[2,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = AUC)) + \n  geom_abline(aes(intercept = model_intercept, slope = model_slope),\n              color = project_colors[2], size = 2) +\n  geom_point(size = 3, alpha = 0.3)\n\n\nThat doesn’t really look like a tight linear relationship, right? If there is any relationship, AUC values become lower with longer reaction times (the line has a negative slope).\n\n\n\n\n\n\nExercise 1e\nNow create a new data frame which contains the extracted posteriors for b_RT from the model output (use the spread_draws() function). Print out the head of the new dataset.\n\n\n\n\n\nShow solution\n# get posteriors for the relevant coefficients\nposteriors1 <- model1 |>\n  # use the spread_draws() function of tidybayes for all relevant parameters\n  spread_draws(b_RT) |>\n  # select only those columns that are relevant\n  select(b_RT) |> \n  # bring into long format\n  gather(key = \"parameter\", value = \"posterior\")\n  \nhead(posteriors1)\n\n\n\n\n\n\n\n\nExercise 1f\nPlot the results with the `geom_halfeyeh() function. Add a vertical line at zero.\n\n\n\n\n\nShow solution\n# plot the regression coefficients\n  ggplot(posteriors1, aes(x = posterior, y = parameter)) + \n    # plot density \n    tidybayes::stat_halfeye(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nb_RT posterior distribution\") +\n    ylab(\"\") +\n    # adjust the x-axis \n    scale_x_continuous(expand = c(0, 0), limits = c(-100,100)) +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\")\n\n\nNow: What is the picture telling us? Is there reason to believe in a relationship between AUC and RT? Think about it!\nThere is no compelling support for a belief in a relationship between AUC and RT. The value zero (no relationship) is contained in the 95% CrI and a non-trivial amount of posterior samples is larger than 0."
  },
  {
    "objectID": "practice-sheets/02a-predictives.html",
    "href": "practice-sheets/02a-predictives.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/02a-predictives.html#new-stuff",
    "href": "practice-sheets/02a-predictives.html#new-stuff",
    "title": "Bayesian regression: theory & practice",
    "section": "New Stuff",
    "text": "New Stuff\nHere is the mouse-tracking data set we used previously for simple linear regression.\n\ndolphin <- aida::data_MT\n# aggregate\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(\n            AUC = median(AUC, na.rm = TRUE),\n            MAD = median(MAD, na.rm = TRUE)) \n\nHere is a plot to remind ourselves.\n\n# plot temperature data\n\ndolphin_agg |> \n  ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(color = project_colors[2])\n\n\n\n\n\n\n\n\n\n\nExercise 3a\nObtain a model fit for AUC ~ MAD with a prior for the slope coefficient as a Student-t distribution with 1 degree of freedom, mean 0 and standard deviation 500.\n\n\n\n\n\nShow solution\nfit_dolphin_agg <- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = prior(student_t(1,0,500), class = \"b\")\n  )\n\n\nHere is how we can extract and plot three samples from the posterior predictive distribution. So, these are three “fake” data sets of the same size and for the same MAD values as in the original data.\n\n# extract & plot posterior predictives\npost_pred <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point() + \n  facet_grid(. ~ run)\n\n\n\n\n\n\n\n\n\n\nExercise 3b\nChange the input to the parameter newdata so that we get three samples for MAD values 400, 500 and 600.\n\n\n\n\n\nShow solution\n# extract & plot posterior predictives\npost_pred2 <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg,\n  newdata = tibble(MAD = c(400, 500, 600)),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point() + \n  facet_grid(. ~ run)\n\n\nWe can also extract predictions for the linear predictor values like so:\n\n# extract & plot posterior linear predictors\n\npost_lin_pred <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line() + \n  facet_grid(. ~ run)\n\n\n\n\n\n\n\n\n\n\nExercise 3c\nExtract 30 samples of linear predictor lines and plot them all in one plot. Make the line plots gray and use a low alpha value (slight transparency).\n\n\n\n\n\nShow solution\npost_lin_pred2 <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 30\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line(aes(group = run), color = \"gray\", alpha = 0.2)\n\n\nFinally, let’s look at a posterior predictive check, based on the distribution of actual / predicted AUC values:\n\npp_check(fit_dolphin_agg, ndraws = 20)\n\n\n\n\n\n\n\n\n\n\nExercise 3d\nRepeat all the steps from the prior predictive point of view for model fit_dolphin_agg,\n\n\n\n\n\nShow solution\nfit_dolphin_agg_prior <- stats::update(fit_dolphin_agg, sample_prior = 'only')\n\npost_pred <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point(alpha = 0.5) + \n  facet_grid(. ~ run)\n\n# extract & plot posterior predictives\npost_pred2 <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = tibble(MAD = c(400, 500, 600)),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point(alpha = 0.8) + \n  facet_grid(. ~ run)\n\n# extract & plot posterior linear predictors\n\npost_lin_pred <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line() + \n  facet_grid(. ~ run)\n\npost_lin_pred2 <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 30\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line(aes(group = run), color = \"gray\", alpha = 0.2)"
  },
  {
    "objectID": "practice-sheets/02c-catPreds-execises.html",
    "href": "practice-sheets/02c-catPreds-execises.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/02c-catPreds-execises.html#exercise-1",
    "href": "practice-sheets/02c-catPreds-execises.html#exercise-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nExercise 1a\nUse brm() to run a linear regression model for the data set dolphin_prepped and the formula:\nlog RT ~ group * condition * prototype_label\nSet the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.\n\n\n\n\n\nShow solution\nfit <- brm(\n  formula = log(RT) ~ group * condition * prototype_label,\n  prior   = prior(student_t(1, 0, 3), class = \"b\"),\n  data    = dolphin_prepped\n  )\n\n\n\n\n\n\n\n\nExercise 1b\nPlot the posteriors for population-level slope coefficients using the tidybayes package in order to:\n\ndetermine which combination of factor levels is the default cell\ncheck which coefficients have 95% CIs that do not include zero\ntry to use this latter information to address any of our research hypotheses (stated above)\n\n\n\n\n\n\nShow solution\ntidybayes::summarise_draws(fit)\ntidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |>\n  filter(.variable != \"b_Intercept\") |>\n  ggplot(aes(y = .variable, x = .value)) +\n  tidybayes::stat_halfeye() +\n  labs(x = \"\", y = \"\") +\n  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n               lty = \"dashed\")\n\n# the default cell is for click-atypical-straight\n\n# coeffiencents with 95% CIs that do not include zero are:\n#   grouptouch, conditionTypical, prototype_labelCoM\n\n# none of these give us direct information about our research hypotheses\n\n\n\n\n\n\n\n\nExercise 1c\nUse the faintr package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.\n\n\n\n\n\nShow solution\n# 1. Typical trials are faster than atypical ones.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = condition == 'Typical',\n  higher = condition == 'Atypical'\n)\n\n# 2. CoM trials are slower than the other kinds of trials\n#    (straight and curved) together, and respectively.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label != 'CoM',\n  higher = prototype_label == 'CoM'\n)\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'CoM'\n)\n\n# 3. 'straight' trials are faster than 'curved' trials.\n# -> There is no evidence for this hypothesis\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'curved'\n)\n\n# 4. Click trials are slower than touch trials.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = group == 'touch',\n  higher = group == 'click'\n)"
  },
  {
    "objectID": "practice-sheets/02c-catPreds-execises.html#exercise-2",
    "href": "practice-sheets/02c-catPreds-execises.html#exercise-2",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nExercise 2a\nCreate a new dataframe that contains only the mean values of the RT, and MAD for each animal (exemplar) and for correct and incorrect responses. Print out the head of the new dataframe.\n\n\n\n\n\nShow solution\n# aggregate\ndolphin_agg <- dolphin |> \n  group_by(exemplar, correct) |> \n  dplyr::summarize(MAD = mean(MAD, na.rm = TRUE),\n                   RT = mean(RT, na.rm = TRUE))\n  \n# let's have a look\nhead(dolphin_agg)\n\n\n\n\n\n\n\n\nExercise 2b\nRun a linear regression using brms. MAD is the dependent variable (i.e. the measure) and both RT and correct are independent variables (MAD ~ RT + correct). Tip: the coefficients might be really really small, so make sure the output is printed with enough numbers after the comma.\n\n\n\n\n\nShow solution\n# specify the model \nmodel2 = brm(\n  # model formula\n  MAD ~ RT + correct, \n  # data\n  data = dolphin_agg\n  )\n\nprint(summary(model2), digits = 5)\n\n\nTry to understand the coefficient table. There is one coefficient for RT and one coefficient for correct which gives you the change in MAD from incorrect to correct responses.\n\n\n\n\n\n\nExercise 2c\nPlot a scatter plot of MAD ~ RT and color code it for correct responses (Tip: Make sure that correct is treated as a factor and not a numeric vector). Draw two predicted lines into the scatterplot. One for correct responses (“lightblue”) and one for incorrect responses (“orange”).\n\n\n\n\n\nShow solution\ndolphin_agg$correct <- as.factor(as.character(dolphin_agg$correct))\n\n# extract model parameters:\nmodel_intercept <- summary(model2)$fixed[1,1]\nmodel_RT <- summary(model2)$fixed[2,1]\nmodel_correct <- summary(model2)$fixed[3,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD,\n           color = correct)) + \n  geom_abline(intercept = model_intercept, slope = model_RT, color = \"orange\", size  = 2) +\n  geom_abline(intercept = model_intercept + model_correct , slope = model_RT, color = \"lightblue\",size  = 2) +\n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\nExercise 2d\nExtract the posteriors for the coefficients of both RT and correct from the model output (use the spread_draws() function), calculate their means and a 67% Credible Interval. Print out the head of the aggregated dataframe.\n\n\n\n\n\nShow solution\n# get posteriors for the relevant coefficients\nposteriors2 <- model2 |>\n  spread_draws(b_RT, b_correct) |>\n  select(b_RT, b_correct) |> \n  gather(key = \"parameter\", value = \"posterior\")\n\n# aggregate\nposteriors2_agg <- posteriors2 |> \n  group_by(parameter) |> \n  summarise(mean_posterior = mean(posterior),\n            `67lowerCrI` = HDInterval::hdi(posterior, credMass = 0.67)[1],\n            `67higherCrI` = HDInterval::hdi(posterior, credMass = 0.67)[2]\n            )\n\n# print out\nposteriors2_agg\n\n\n\n\n\n\n\n\nExercise 2e\nPlot the scatterplot from 2c and plot 50 sample tuples for the regression lines for correct and incorrect responses.\n\n\n\n\n\nShow solution\n# sample 50 random numbers from 4000 samples\nrandom_50 <- sample(1:4000, 50, replace = FALSE)\n  \n# wrangle data frame\nposteriors3 <- model2 |>\n  spread_draws(b_Intercept, b_RT, b_correct) |>\n  select(b_Intercept, b_RT, b_correct) |> \n  # filter by the row numbers in random_50\n  slice(random_50)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD, \n           color = correct)) + \n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept, slope = b_RT), \n              color = \"orange\", size  = 0.1) +\n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept + b_correct, slope = b_RT), \n              color = \"lightblue\", size  = 0.1) +\n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\nExercise 2f\nGiven our model and our data, calculate the evidential ratio of correct responses exhibiting larger MADs than incorrect responses.\n\n\n\n\n\nShow solution\nhypothesis(model2, 'correct > 0')"
  },
  {
    "objectID": "practice-sheets/02c-catPreds-execises.html#exercise-3",
    "href": "practice-sheets/02c-catPreds-execises.html#exercise-3",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercise 3",
    "text": "Exercise 3\nHere is an aggregated data set dolphin_agg for you.\n\n# aggregate\ndolphin_agg <- dolphin %>% \n  group_by(group, exemplar) %>% \n  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),\n                   RT = median(RT, na.rm = TRUE)) %>% \n  mutate(log_RT = log(RT))\n\n\n\n\n\n\n\nExercise 3a\nStandardize (“z-transform”) log_RT such that the mean is at zero and 1 unit corresponds to the standard deviation. Name it log_RT_s.\n\n\n\n\n\nShow solution\ndolphin_agg$log_RT_s <- scale(dolphin_agg$log_RT, scale = TRUE)\n\n\n\n\n\n\n\n\nExercise 3b\nRun a linear model with brms that predicts MAD based on log_RT_s, group, and their two-way interaction.\n\n\n\n\n\nShow solution\nmodel1 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg\n  )\n\n\n\n\n\n\n\n\nExercise 3c\nPlot MAD (y) against log_RT_s (x) in a scatter plot and color-code for group. Plot the regression lines for the click and the touch group into the plot and don’t forget to take possible interactions into account.\n\n\n\n\n\nShow solution\n# extract posterior means for model coefficients\nIntercept = summary(model1)$fixed[1,1]\nlog_RT = summary(model1)$fixed[2,1]\ngroup = summary(model1)$fixed[3,1]\ninteraction = summary(model1)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[2], size = 2) \n\n\n\n\n\n\n\n\nExercise 3d\nSpecify very skeptic priors for all three coefficients. Use a normal distribution with mean = 0, and sd = 10. Rerun the model with those priors.\n\n\n\n\n\nShow solution\n# specify priors\npriors_model2 <- c(\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"grouptouch\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s:grouptouch\")\n)\n\n# model\nmodel2 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg,\n  prior = priors_model2\n  )\n\n\n\n\n\n\n\n\nExercise 3e\nCompare the model output of model1 to model2. What are the differences and what are the reasons for these differences?\n\n\n\n\n\nShow solution\n# We can compare the model predictions by looking at the coefficients / plotting them:\nsummary(model1)\nsummary(model2)\n\n# extract posterior means for model coefficients\nIntercept = summary(model2)$fixed[1,1]\nlog_RT = summary(model2)$fixed[2,1]\ngroup = summary(model2)$fixed[3,1]\ninteraction = summary(model2)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[1], size = 2) \n\n# ANSWER: the magnitude of the coefficients is much smaller in model2, with the interaction term being close to zero. As a result, the lines in the plot are closer together and run in parallel. The reason for this change lies in the priors. We defined the priors of model2 rather narrowly, down weighing  data points larger or smaller than zero. This is a case of the prior dominating the posterior."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html",
    "href": "practice-sheets/01a-wrangling-plotting.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#loading-and-inspecting-the-data",
    "href": "practice-sheets/01a-wrangling-plotting.html#loading-and-inspecting-the-data",
    "title": "Bayesian regression: theory & practice",
    "section": "Loading and inspecting the data",
    "text": "Loading and inspecting the data\nThe data is part of the aida package, but we can give it a fancy new name:\n\ndolphin <- aida::data_MT\n\nTo get some information about the data set, we can use the help function:\n\nhelp(\"data_MT\")\n\nHere is some more information we can get about the data:\n\n# number of rows in the data set\nnrow(dolphin)\n\n[1] 2052\n\n# number of columns in the data set\nncol(dolphin)\n\n[1] 16\n\n# names of the columns\nnames(dolphin)\n\n [1] \"X1\"               \"trial_id\"         \"MAD\"              \"AUC\"             \n [5] \"xpos_flips\"       \"RT\"               \"prototype_label\"  \"subject_id\"      \n [9] \"group\"            \"condition\"        \"exemplar\"         \"category_left\"   \n[13] \"category_right\"   \"category_correct\" \"response\"         \"correct\"         \n\n# number of unique `subject_id`s\ndolphin$subject_id |> unique() |> length()\n\n[1] 108\n\n# number of types each subject saw different `conditions`\ndolphin |> with(table(subject_id, condition)) |> head()\n\n          condition\nsubject_id Atypical Typical\n      1001        6      13\n      1002        6      13\n      1003        6      13\n      1004        6      13\n      1005        6      13\n      1006        6      13"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#a-closer-look-at-the-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#a-closer-look-at-the-columns",
    "title": "Bayesian regression: theory & practice",
    "section": "A closer look at the columns",
    "text": "A closer look at the columns\nLet’s take a closer look at the columns and the information inside them.\nWe can get a glimpse of all columns like so:\n\nglimpse(dolphin)\n\nRows: 2,052\nColumns: 16\n$ X1               <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ trial_id         <chr> \"id0001\", \"id0002\", \"id0003\", \"id0004\", \"id0005\", \"id…\n$ MAD              <dbl> 82.53319, 44.73484, 283.48207, 138.94863, 401.93988, …\n$ AUC              <dbl> 40169.5, 13947.0, 84491.5, 74084.0, 223083.0, 308376.…\n$ xpos_flips       <dbl> 3, 1, 2, 0, 2, 2, 1, 0, 2, 0, 2, 2, 0, 0, 3, 1, 0, 1,…\n$ RT               <dbl> 950, 1251, 930, 690, 951, 1079, 1050, 830, 700, 810, …\n$ prototype_label  <chr> \"straight\", \"straight\", \"curved\", \"curved\", \"cCoM\", \"…\n$ subject_id       <dbl> 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001,…\n$ group            <chr> \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\",…\n$ condition        <chr> \"Atypical\", \"Typical\", \"Atypical\", \"Atypical\", \"Typic…\n$ exemplar         <chr> \"eel\", \"rattlesnake\", \"bat\", \"butterfly\", \"hawk\", \"pe…\n$ category_left    <chr> \"fish\", \"amphibian\", \"bird\", \"Insekt\", \"bird\", \"fish\"…\n$ category_right   <chr> \"reptile\", \"reptile\", \"mammal\", \"bird\", \"reptile\", \"b…\n$ category_correct <chr> \"fish\", \"reptile\", \"mammal\", \"Insekt\", \"bird\", \"bird\"…\n$ response         <chr> \"fish\", \"reptile\", \"bird\", \"Insekt\", \"bird\", \"bird\", …\n$ correct          <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,…\n\n\nHere is a quick explanation of all the different columns:\n\ntrial_id = unique id for individual trials\nMAD = maximal deviation into competitor space\nAUC = area under the curve\nxpos_flips = the amount of horizontal direction changes\nRT = reaction time in ms\nprototype_label = different categories of prototypical movement strategies\nsubject_id = unique id for individual participants\ngroup = groups differ in the response design (click vs. touch)\ncondition = category membership (Typical vs. Atypical)\nexemplar = the concrete animal\ncategory_left = the category displayed on the left\ncategory_right = the category displayed on the right\ncategory_correct= the category that is correct\nresponse = the selected category\ncorrect = whether or not the response matches category_correct"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#selecting-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#selecting-columns",
    "title": "Bayesian regression: theory & practice",
    "section": "Selecting columns",
    "text": "Selecting columns\nFor now, we are only interested in columns RT, group, condition, category_correct, and correct. We can use the select() function of dplyr to get rid of columns we don’t need.\n\n# selecting specific columns\ndolphin_selected <-\n  dolphin |>\n  dplyr::select(RT, group, condition, category_correct, correct)\n \n# let's have a look\ndolphin_selected\n\n# A tibble: 2,052 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5   951 touch Typical   bird                   1\n 6  1079 touch Atypical  bird                   1\n 7  1050 touch Typical   fish                   1\n 8   830 touch Typical   fish                   1\n 9   700 touch Typical   mammal                 1\n10   810 touch Typical   fish                   1\n# … with 2,042 more rows"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#filtering-rows",
    "href": "practice-sheets/01a-wrangling-plotting.html#filtering-rows",
    "title": "Bayesian regression: theory & practice",
    "section": "Filtering rows",
    "text": "Filtering rows\nIf we care only about a subset of rows, we can use the filter() function. For example, let’s filter all trials in which the correct category was either a fish or a mammal\n\ndolphin_filter1 <-\n  dolphin_selected |> \n  filter(category_correct == \"fish\" | category_correct == \"mammal\")\n  # the | is a logical operator that indicates that either the first expression OR \n  # the second one has to be true\n\ndolphin_filter1\n\n# A tibble: 1,296 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2   930 touch Atypical  mammal                 0\n 3  1050 touch Typical   fish                   1\n 4   830 touch Typical   fish                   1\n 5   700 touch Typical   mammal                 1\n 6   810 touch Typical   fish                   1\n 7  1264 touch Typical   mammal                 1\n 8   890 touch Atypical  mammal                 0\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# … with 1,286 more rows\n\n\nYou can also filter() against particular conditions. For example, let’s filter all rows that do not have bird as their correct category:\n\ndolphin_filter2 <-\n  dolphin_selected |> \n  filter(category_correct != \"bird\")\n\ndolphin_filter2\n\n# A tibble: 1,728 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5  1050 touch Typical   fish                   1\n 6   830 touch Typical   fish                   1\n 7   700 touch Typical   mammal                 1\n 8   810 touch Typical   fish                   1\n 9  1264 touch Typical   mammal                 1\n10   890 touch Atypical  mammal                 0\n# … with 1,718 more rows\n\n\nWe can also filter according to multiple conditions at once, including numeric conditions. Here, we also filter for trials that have correct responses.\n\ndolphin_filter3 <-\n  dolphin_selected |> \n  filter(category_correct != \"bird\",\n         correct == 1)\n\ndolphin_filter3\n\n# A tibble: 1,602 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   690 touch Atypical  Insekt                 1\n 4  1050 touch Typical   fish                   1\n 5   830 touch Typical   fish                   1\n 6   700 touch Typical   mammal                 1\n 7   810 touch Typical   fish                   1\n 8  1264 touch Typical   mammal                 1\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# … with 1,592 more rows"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#grouping-and-summarizing",
    "href": "practice-sheets/01a-wrangling-plotting.html#grouping-and-summarizing",
    "title": "Bayesian regression: theory & practice",
    "section": "Grouping and summarizing",
    "text": "Grouping and summarizing\nWe can also generate summary statistics of certain variables with a combination of group_by() & summarise(). Let’s get the means and standard deviations of the reactions times for each level in the variable condition. We also include the minimum and maximum values for each condition.\n\ndolphin_aggregate <-\n  dolphin_filter3 |>\n  group_by(condition) |>\n  summarise(\n    min_RT  = min(RT),\n    mean_RT = mean(RT, na.rm = T),\n    sd_RT   = sd(RT, na.rm = T),\n    max_RT  = max(RT)\n    )\n  # the na.rm = T is an argument that is used to tell R that NAs should be ignored \n  # when calculating the summary statistics\n\n# show the aggregated df\ndolphin_aggregate\n\n# A tibble: 2 × 5\n  condition min_RT mean_RT sd_RT max_RT\n  <chr>      <dbl>   <dbl> <dbl>  <dbl>\n1 Atypical     630   2149. 1840.  19903\n2 Typical      510   1665. 1283.  20685\n\n\nSo we find that atypical categories are responded to slower than typical categories. Makes sense. Identifying a dolphin as a mammal might be difficult because it shares a lot of features with fish.\nWe can group according to many different factors simultaneously, and we can create multiple summary statistics at the same time. Here, we get summary statistics for each combination of all levels in variables condition and group. We use the tidyboot package to get bootstrapped 95% confidence intervalls. (Notice that these are more informative than standard deviations in the sense that they give an upper and lower deviation, not just one number for both directions, which can be misleading when the data is skewed (like reaction times typically are)):\n\ndolphin_aggregate2 <-\n  dolphin_filter3 |>\n  group_by(group, condition) |>\n  summarize(\n    lower_CI = tidyboot::ci_lower(RT),\n    mean_RT  = mean(RT, na.rm = T),\n    upper_CI = tidyboot::ci_upper(RT)\n    )\n\n# show the aggregated df\ndolphin_aggregate2\n\n# A tibble: 4 × 5\n# Groups:   group [2]\n  group condition lower_CI mean_RT upper_CI\n  <chr> <chr>        <dbl>   <dbl>    <dbl>\n1 click Atypical     1030.   2417.    7311.\n2 click Typical       950    1847.    4698.\n3 touch Atypical      750    1900.    5368.\n4 touch Typical       686.   1486.    3955.\n\n\nWe can see here that the group that needed to click on a response are overall slower than touch responses, but also much more variable in their behavior."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#changing-and-adding-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#changing-and-adding-columns",
    "title": "Bayesian regression: theory & practice",
    "section": "Changing and adding columns",
    "text": "Changing and adding columns\nOften, we are interested in standardized measures because we do not know what a value of 1 means on any given scale. Is 1 a large difference or a small difference? For example when we want to explore the impact of several predictors on the same measurement, we want to know the relative size of a number. To achieve this, we standardize measures by dividing their mean by their respective standard deviations. We will use the scale() function for this and create a new variable in our data frame via mutate().\n(Note that the scale() function creates an object that is of the matrix class. That is fine for the most part but might create issues later on. To avoid any issues, we wrap the scale() function in as.numeric() to store the results as a numeric vector.)\n\ndolphin_standardize <-\n  dolphin_selected |>\n  mutate(RT_scale = as.numeric(scale(RT)))\n  \nhead(dolphin_standardize)\n\n# A tibble: 6 × 6\n     RT group condition category_correct correct RT_scale\n  <dbl> <chr> <chr>     <chr>              <dbl>    <dbl>\n1   950 touch Atypical  fish                   1   -0.552\n2  1251 touch Typical   reptile                1   -0.373\n3   930 touch Atypical  mammal                 0   -0.564\n4   690 touch Atypical  Insekt                 1   -0.706\n5   951 touch Typical   bird                   1   -0.551\n6  1079 touch Atypical  bird                   1   -0.475\n\n\nIf we now compare, say atypical and typical categories according to reaction times, we can use the standardized RT ratings. Let’s do all of this in one “pipeline”.\n\ndolphin_agg_standardize <- dolphin_selected |>\n  mutate(RT_scale = scale(RT)) |> \n  group_by(condition) |>\n  summarise(mean_RT_scale = mean(RT_scale, na.rm = T))\n  \nhead(dolphin_agg_standardize)\n\n# A tibble: 2 × 2\n  condition mean_RT_scale\n  <chr>             <dbl>\n1 Atypical          0.219\n2 Typical          -0.101\n\n\nNow we can see that atypical categories exhibit relatively higher RTs, i.e., more than 0.3 standard deviations higher than for typical categories."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#exercises-for-data-wrangling",
    "href": "practice-sheets/01a-wrangling-plotting.html#exercises-for-data-wrangling",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises for data wrangling",
    "text": "Exercises for data wrangling\n\nExercise 1\n\n\n\n\n\n\nExercise 1a\nTake the dolphin data set and store a reduced variant of it as dolphin_reduced. The new data frame should contain only the following columns: RT, AUC, group, and exemplar.\n\n\n\n\n\nShow solution\ndolphin_reduced <- dolphin |>\n  select(RT, AUC, group, exemplar)\n\nhead(dolphin_reduced)\n\n\n\n\n\n\n\n\nExercise 1b\nWe are for now only interested in those data that have whales as the exemplar. filter() only those rows and store them in a new dataframe called whales_only.\n\n\n\n\n\nShow solution\nwhales_only <- dolphin_reduced |> \n  filter(exemplar == \"whale\")\n\nhead(whales_only)\n\n\n\n\n\n\n\n\nExercise 1c\nNow filter for only those data that have RTs below 1500ms.\n\n\n\n\n\nShow solution\nwhales_only2 <- whales_only |> \n  filter(RT < 1500)\n\nhead(whales_only2)\n\n\n\n\n\n\n\n\nExercise 1d\nWe don’t like that AUC is unstandardized. Use mutate() to create a new vector that represents scaled AUC values (scaling is achieved by the function scale()).\n\n\n\n\n\nShow solution\nwhales_only_scaled <- whales_only2 |> \n  mutate(AUC_scaled = scale(AUC))\n\nhead(whales_only_scaled)\n\n\n\n\n\n\n\n\nExercise 1e\nCalculate the mean scaled AUC ratings for both both groups.\n\n\n\n\n\nShow solution\nwhales_aggregate <- whales_only_scaled |> \n  group_by(group) |> \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n\nhead(whales_aggregate)\n\n\n\n\n\n\n\n\nExercise 1f\nDo all of the above (a-e) in one pipeline.\n\n\n\n\n\nShow solution\nwhales_aggregate <- dolphin |>\n  select(RT, AUC, group, exemplar) |> \n  filter(exemplar == \"whale\",\n         RT < 1500) |> \n  mutate(AUC_scaled = scale(AUC)) |> \n  group_by(group) |> \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n  \nhead(whales_aggregate)\n\n\n\n\nExercise 2\n\n\n\n\n\n\nExercise 1a\nTake the dolphin data set and store a reduced variant of it. The new data frame should contain only the columns condition, group, and xpos_flips, correct. And within the correct vector, we are only interested in the correct trials (= 1). Filter accordingly.\n\n\n\n\n\nShow solution\ndolphin_sub <- dolphin |> \n  select(condition, group, xpos_flips, correct) |> \n  filter(correct == 1)\n\nhead(dolphin_sub)\n\n\n\n\n\n\n\n\nExercise 2b\nCreate an aggregated data frame that contains the mean xpos_flips value and the standard deviation for group and condition.\n\n\n\n\n\nShow solution\ndolphin_agg <- dolphin_sub |>\n  group_by(group, condition) |> \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE))\n\nhead(dolphin_agg)\n\n\n\n\n\n\n\n\nExercise 2c\nUse the rename() function to rename the new vectors for the mean xflips and their standard deviation to xflips_mean and xflips_sd.\n\n\n\n\n\nShow solution\ndolphin_agg2 <- dolphin_agg |>\n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)\n\n\n\n\n\n\n\n\nExercise 2d\nDo all of the above (a-c) in one pipeline.\n\n\n\n\n\nShow solution\ndolphin |> \n  select(condition, group, xpos_flips, correct) |> \n  filter(correct == 1) |> \n  group_by(group, condition) |> \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE)) |> \n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#basic-plots",
    "href": "practice-sheets/01a-wrangling-plotting.html#basic-plots",
    "title": "Bayesian regression: theory & practice",
    "section": "Basic plots",
    "text": "Basic plots\nNow that we have pre-processed our data set, we are ready to visually explore it. Let’s start very simple. Let’s plot a bar plot. Let’s also add a title to our plot.\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"a bare bar plot\")\n\n\n\n\n\n\n\n  # stat = \"identity\" takes the number in the dataset as the bar height (as opposed to a 'count')\n\nUgh! What an ugly plot, right? But it’s already telling a story: Atypical categories are responded to slower than typical categories. Let’s add a measure of uncertainty, in our case the bootstrapped 95% confidence intervals, as error bars to the plot:\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") + \n  \n  # this is the added layer\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\",\n                linewidth = 0.5) +\n  \n  ggtitle(\"a bare bar plot with error bars\")\n\n\n\n\nWe can observe a couple of things here. First, ggplot automatically adjust the axes based on the elements to be plotted unless we tell it not to. Second, the error bars are plotted in front of the bars, i.e. closer to the viewer. This visual ordering reflects the order of layers. We first plotted the bars and THEN the error bars.\nBeyond bar plots, we can create other useful plots types. For example a point plot. Instead of a bar, we plot the mean RT as points.\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\") +  \n  # this is the new geom \n  geom_point() +\n  ggtitle(\"a point plot\")\n\n\n\n\n\n\n\n\nOr a line plot that connects the means with a line. For the line plot to work, we need to indicate a group aesthetic, i.e. the group that we want to connect with a line. If you have for example several interacting categories, you need to indicate which groups are supposed to be connected with lines (see below). Because we have only one group here, condition, we set group to 1.\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT, group = 1)) +\n  geom_line() +\n  ggtitle(\"a line plot\")\n\n\n\n\n\n\n\n\nYay, we are on a roll. Let’s plot a box plot. Remember the box shows the median (middle vertical line) and the interquartile range (the middle 50% of the data within the box). Note that for the box plot, we do not plot aggregated values, so we need to refer to the entire data set. We also add the aesthetic fill here and set it to the variable condition to color code our boxes.\n\n# we changed the dataset referred to\nggplot(dolphin, aes(x = condition, y = RT, fill = condition)) +\n  # this is the new geom \n  geom_boxplot() +\n  ggtitle(\"a box plot\")\n\n\n\n\n\n\n\n\nWhile the above plots illustrate one continuous variable (RT) plotted against a categorical variable (condition), we can also plot two continuous variables against each other. For example, we could plot RT against AUC in a scatter plot.\n\n# we changed the y aesthetic to `Hardness`\nggplot(dolphin_subset, aes(x = RT, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\nFinally, one central plot type for our class. The density plot. It plots on the x-axis a continous value and on the y-axis the “density”” of these values. So high values on the y-axis means a lot of data at the corresponding x-values. The density curve can be outlined with color and filled with fill. To keep the two categories visually distinct, we add an argument to the geom_density() function: alpha. Alpha controls the transparency of the color! We will see a lot of these density plots in our class.\n\nggplot(dolphin, aes(x = RT, color = condition, fill = condition)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"a density plot\")"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#adjusting-plot-elements",
    "href": "practice-sheets/01a-wrangling-plotting.html#adjusting-plot-elements",
    "title": "Bayesian regression: theory & practice",
    "section": "Adjusting plot elements",
    "text": "Adjusting plot elements\nOkay, so we are now already capable of exploring our data visually with a bunch of plots. These plots are exceptionally ugly and of limited communicative value so far. Note that this is perfectly fine during an exploratory phase of data analysis. If we just eye-ball data and we have no trouble interpreting the plots, thats just fine. However, as soon as we want to communicate patterns to others with these graphs, we need to take a little bit more care of its communicative value. Let’s look at ways we can tailor our plots to the needs of an audience.\nLet’s go back to our bar plot and explore whether condition and group has an impact on RT?\n\n# First we aggregate RT for group and condition\ndolphin_agg2 <- dolphin_subset |>\n  group_by(group, condition) |>\n  summarise(mean_RT = mean(RT),\n            sd_RT = sd(RT))\n\n# then we plot and color code for condition (note that, for bar plots, the aesthetic of `color` refers to the border of the bar, and `fill` refers to the actual colour of the bar)\n\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nHm… that doesn’t work out, the bars are on top of each other, we need to tell ggplot to position the bars next to each other instead. We do that with position_dogde(). Note that ggplot assigns a default color coding scheme to your plots if you don’t specify it by hand.\n\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\", position = position_dodge())\n\n\n\n\n\n\n\n\nAwww much better! Alternatively, we can plot the two categories into separate panels. We achieve this by facetting using the facet_grid() function.\n\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  # this is the facetting function\n  facet_grid(~ condition)\n\n\n\n\n\n\n\n\nOkay. We are getting somewhere. We already learned something again. Apparently, the effect of typiciality on RT is pretty similar across tasks. It does not look like we have an interaction here (more on interactions later).\nNow let’s make these plots ready to communicate information. We add appropriate axes titles. Note that we are using a little hack here: The “” inserts an empty line, creating visual distance from axis title to axis, thus making it easier to read it. Our audience will thank us.\n\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  facet_grid(~ condition) +\n  # add axis titles\n  xlab(\"\\n task\") +\n    ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n\nThe same graph as a point / line plot indicated to the audience whether there is an interaction pattern or not. Note that here we do not facet because we actually want the points to be plotted within the same horizontal space. We also have to specify the group aesthetic to tell ggplot which points to connect with lines.\n\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, color = condition, group = condition)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 12) +\n  geom_line(size = 2) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n  # # need to change to the color aesthetic instead of fill\n  # scale_y_continuous(expand = c(0, 0), breaks = (c(0, 500, 1000, 1500, 2000, 2500, 3000)), limits = c(0,3000))\n\nThese lines look pretty parallel and don’t indicate a strong interaction pattern. But how do different exemplars differ? Let’s aggregate for individual exemplars first and then create the same plot for the means of all exemplars.\n\ndolphin_agg3 <- dolphin_subset |> \n  group_by(exemplar, group, condition) |> \n  summarise(mean_RT = mean(RT, na.rm = TRUE))\n\nggplot(dolphin_agg3, aes(x = group, y = mean_RT, color = condition, group = exemplar)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 6, alpha = 0.3) +\n  geom_line() +\n  geom_label(aes(label = exemplar)) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\")\n\n\n\n\n\n\n\n\nIt looks like “shark” and “rattlesnake” behave very different from their buddies in the typical condition. Interesting! We wouldn’t have noticed if we had only looked at the overall means."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#exercises-for-plotting",
    "href": "practice-sheets/01a-wrangling-plotting.html#exercises-for-plotting",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises for plotting",
    "text": "Exercises for plotting\nTake the scatter plot below as a departure point. It plots AUC (area-under-the-curve) against MAD (maximal absolute deviation).\n\nggplot(dolphin, aes(x = MAD, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\n\n\nExercise 3a\n\nChange both the x-axis and the y-axis title to sensible and informative titles.\nChange the plot title to something informative.\nChange the scaling of the x-axis to display only MAD values between -500 and 500\n\n\n\n\n\n\nShow solution\nggplot(dolphin, aes(x = MAD, y = AUC, \n                           \n                           # (d) add color aesthetic\n                           color = group)) +\n  geom_point() +\n  \n  # (1) axes titles\n  xlab(\"\\n maximal absolute deviation\") +\n  ylab(\"area-under-the-curve \\n\") +\n  \n  # (2) change title\n  ggtitle(\"MAD is correlated with AUC\") +\n\n  # (3) change x-axis (note that certain values are not displayed then. R will spit out a warning)\n  scale_x_continuous(limits = c(-500,500))\n\n\n\n\n\n\n\n\nExercise 4\n\nPlot AUC values as a function of group in a density plot (geom_density).\n\n(2)) Make the density curves semi-transparent with the alpha argument\n(3)) Add the aida_theme to the plot.\n\nAdd the mean values for both groups into the density plot as a line.\n\n\n\n\n\n\nShow solutions for a-c\n# (1 - 3)\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\")\n\n\n\n\nShow solution for d\n# (4) aggregate means and add to plot\n\ndolphin_agg <- dolphin |>\n  group_by(group) |>\n  summarise(mean_AUC = mean(AUC, na.rm = TRUE),\n            sd_AUC = sd(AUC, na.rm = TRUE))\n\n# add them to the plot as vertical lines\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\") +\n  # since the vertical line refers to dolphin_agg, we need to specify the dataset explicitly \n  geom_vline(data = dolphin_agg, \n             aes(xintercept = mean_AUC, color = group),\n             lty = \"dashed\")"
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The first session introduces the basics of BDA and linear regression modeling.\nHere are slides for session 1.\nThere are two hands-on exploration and exercise sheets. The first is a crash-course (or recap) of basic wrangling and plotting in the tidyverse. Here, we will also have a first rendez-vous with the data set (mouse-tracking data) which most of our hands-on monkeying will be based on. The second sheet covers Bayesian simple linear regression (using brms)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This site provides material for an introductory-level course on Bayesian linear regression modeling. The course presupposes some prior exposure to statistics and some acquaintance with R."
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Intended audience",
    "text": "Intended audience\nThis course is offered to students (both under-graduate and graduate) of (computational) linguistics and cognitive science."
  },
  {
    "objectID": "index.html#scope",
    "href": "index.html#scope",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Scope",
    "text": "Scope\nThe course introduces the basics of Bayesian data analysis, and then immedidately zooms in on regression modeling. Starting with simple linear regression, we want to reach a basic understanding of hierarchical generalized linear models."
  },
  {
    "objectID": "index.html#additional-material",
    "href": "index.html#additional-material",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Additional material",
    "text": "Additional material\nAn even more basic introduction to data analysis (introducing R, tidyvers, Bayesian and, eventually, also frequentist statistics) is the webbook “An introduction to Data Analysis”."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of this material was used in a previous course, co-taught with the great Timo Roettger. I’m recycling some of his material here, but have (in full awareness of the catastrohpe) made it more dry, less insightful and more nerdy to conceal the breach of authorship."
  },
  {
    "objectID": "02-priorPredictives-categPredictors.html",
    "href": "02-priorPredictives-categPredictors.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "Priors are an essential part of Bayesian data analysis. There are different approaches to specify priors:\n\nwe can try to be feign maximal uncertainty;\nwe can try to commit strongly to prior knowledge we think is relevant;\nwe can use priors to make the model “well-behaved” (e.g., help training / fitting)\nwe can try to use priors that are as “objective” and “non-committal”.\n\nWhatever approach to specifying priors we adopt, all (proper) priors generate predictions. So here we look at prior and posterior predictives of a Bayesian model. This is helpful because you might not have intuitions about how to select priors, but you will likely have intuitions about what counts are reasonable /a priori/ predictions.\nHere are slides for session 2.\n\n\nIn the second part of this session, we look at categorical predictors in simple linear regression models.\nMore on this topic can be found in this chapter of the webbook."
  }
]